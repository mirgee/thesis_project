{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jan 30 2019 \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.2.0\n",
      "\n",
      "numpy 1.15.4\n",
      "scipy 1.2.0\n",
      "sklearn 0.20.2\n",
      "pandas 0.24.0\n",
      "\n",
      "compiler   : GCC 7.3.0\n",
      "system     : Linux\n",
      "release    : 4.20.5-arch1-1-ARCH\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /home/kovar/thesis_project/data/processed/1a.fif...\n",
      "This filename (/home/kovar/thesis_project/data/processed/1a.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "Isotrak not found\n",
      "    Range : 0 ... 19104 =      0.000 ...    76.416 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d3665bdb3882>:31: RuntimeWarning: This filename (/home/kovar/thesis_project/data/processed/1a.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "  raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '1a.fif'))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import nolds\n",
    "import data\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "\n",
    "PROJ_ROOT = os.path.abspath(os.path.join(os.pardir))\n",
    "DATA_ROOT = os.path.abspath(os.path.join(PROJ_ROOT, 'data'))\n",
    "PROCESSED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'processed'))\n",
    "RAW_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'raw'))\n",
    "LABELED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'labeled'))\n",
    "DURATIONS_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'durations'))\n",
    "REC_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'recplots'))\n",
    "DIRECT_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'direct'))\n",
    "GAF_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'gaf'))\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "CHANNEL_NAMES = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "                 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "META_COLUMN_NAMES = ['freq', 'RESP_4W', 'RESP_FIN', 'REMISE_FIN', 'AGE', 'SEX', 'M_1',\n",
    "       'M_4', 'M_F', 'délka léčby', 'lék 1', 'lék 2', 'lék 3', 'lék 4']\n",
    "META_FILE_NAME = 'DEP-POOL_Final_144.xlsx'\n",
    "# meta_df = pd.read_excel(os.path.join(RAW_ROOT, META_FILE_NAME), index_col='ID', names=META_COLUMN_NAMES)\n",
    "\n",
    "raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '1a.fif'))\n",
    "t = pd.DataFrame(raw_fif.get_data())\n",
    "data = pd.DataFrame(np.transpose(t.values), columns=CHANNEL_NAMES)\n",
    "data = np.transpose(data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os.path\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from braindecode.experiments.experiment import Experiment\n",
    "from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or\n",
    "from braindecode.datautil.iterators import BalancedBatchSizeIterator\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from braindecode.models.deep4 import Deep4Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welford's algorithm for computing running mean and variance\n",
    "def update(existingAggregate, newValues):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    for newValue in newValues: \n",
    "        count += 1\n",
    "        delta = newValue - mean\n",
    "        mean += delta / count\n",
    "        delta2 = newValue - mean\n",
    "        M2 += delta * delta2\n",
    "        existingAggregate = (count, mean, M2)\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "def finalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance) = (mean, M2/count) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.signalproc import bandpass_cnt\n",
    "low_cut_hz = 4 # Suggested 0 or 4\n",
    "high_cut_hz = 38\n",
    "def compute_sep(file, path, ww=256, maxl=np.infty, rescale=True, filter=True):\n",
    "    minl = ww\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    N = len(file.df['FP1'].values)\n",
    "    while start+ww <= min(maxl, N):\n",
    "        ret = np.zeros((len(CHANNEL_NAMES), ww))\n",
    "        for i, channel in enumerate(CHANNEL_NAMES):\n",
    "            data = file.df[channel].values\n",
    "            r = data[start:start+ww]\n",
    "            if 2*len(data) < minl + ww or r.shape[0]*r.shape[0] != ww*ww:\n",
    "                print('Skipping, file ', file.name, ', time series too short: ', len(data))\n",
    "                print('Or returned wrong shape: ', r.shape, start+ww)\n",
    "                continue\n",
    "            if rescale:\n",
    "                r *= 1e6\n",
    "            if filter:\n",
    "                r = bandpass_cnt(r, low_cut_hz, high_cut_hz, 250, filt_order=3, axis=0)\n",
    "            counts[i], means[i], M2s[i] = update((counts[i], means[i], M2s[i]), r)\n",
    "            ret[i, :] = r\n",
    "        np.save(\n",
    "            os.path.join(path, ''.join((str(file.id), file.trial, '-', str(chunk_num), '.npy'))),\n",
    "            file.df.values.T[:, start:start+ww], fix_imports=False)\n",
    "        start += ww\n",
    "        chunk_num += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0\n",
      "Processed:  1\n",
      "Processed:  2\n",
      "Processed:  3\n",
      "Processed:  4\n",
      "Processed:  5\n",
      "Processed:  6\n",
      "Processed:  7\n",
      "Processed:  8\n",
      "Processed:  9\n",
      "Processed:  10\n",
      "Processed:  11\n",
      "Processed:  12\n",
      "Processed:  13\n",
      "Processed:  14\n",
      "Processed:  15\n",
      "Processed:  16\n",
      "Processed:  17\n",
      "Processed:  18\n",
      "Processed:  19\n",
      "Processed:  20\n",
      "Processed:  21\n",
      "Processed:  22\n",
      "Processed:  23\n",
      "Processed:  24\n",
      "Processed:  25\n",
      "Processed:  26\n",
      "Processed:  27\n",
      "Processed:  28\n",
      "Processed:  29\n",
      "Processed:  30\n",
      "Processed:  31\n",
      "Processed:  32\n",
      "Processed:  33\n",
      "Processed:  34\n",
      "Processed:  35\n",
      "Processed:  36\n",
      "Processed:  37\n",
      "Processed:  38\n",
      "Processed:  39\n",
      "Processed:  40\n",
      "Processed:  41\n",
      "Processed:  42\n",
      "Processed:  43\n",
      "Processed:  44\n",
      "Processed:  45\n",
      "Processed:  46\n",
      "Processed:  47\n",
      "Processed:  48\n",
      "Processed:  49\n",
      "Processed:  50\n",
      "Processed:  51\n",
      "Processed:  52\n",
      "Processed:  53\n",
      "Processed:  54\n",
      "Processed:  55\n",
      "Processed:  56\n",
      "Processed:  57\n",
      "Processed:  58\n",
      "Processed:  59\n",
      "Processed:  60\n",
      "Processed:  61\n",
      "Processed:  62\n",
      "Processed:  63\n",
      "Processed:  64\n",
      "Processed:  65\n",
      "Processed:  66\n",
      "Processed:  67\n",
      "Processed:  68\n",
      "Processed:  69\n",
      "Processed:  70\n",
      "Processed:  71\n",
      "Processed:  72\n",
      "Processed:  73\n",
      "Processed:  74\n",
      "Processed:  75\n",
      "Processed:  76\n",
      "Processed:  77\n",
      "Processed:  78\n",
      "Processed:  79\n",
      "Processed:  80\n",
      "Processed:  81\n",
      "Processed:  82\n",
      "Processed:  83\n",
      "Processed:  84\n",
      "Processed:  85\n",
      "Processed:  86\n",
      "Processed:  87\n",
      "Processed:  88\n",
      "Processed:  89\n",
      "Processed:  90\n",
      "Processed:  91\n",
      "Processed:  92\n",
      "Processed:  93\n",
      "Processed:  94\n",
      "Processed:  95\n",
      "Processed:  96\n",
      "Processed:  97\n",
      "Processed:  98\n",
      "Processed:  99\n",
      "Processed:  100\n",
      "Processed:  101\n",
      "Processed:  102\n",
      "Processed:  103\n",
      "Processed:  104\n",
      "Processed:  105\n",
      "Processed:  106\n",
      "Processed:  107\n",
      "Processed:  108\n",
      "Processed:  109\n",
      "Processed:  110\n",
      "Processed:  111\n",
      "Processed:  112\n",
      "Processed:  113\n",
      "Processed:  114\n",
      "Processed:  115\n",
      "Processed:  116\n",
      "Processed:  117\n",
      "Processed:  118\n",
      "Processed:  119\n",
      "Processed:  120\n",
      "Processed:  121\n",
      "Processed:  122\n",
      "Processed:  123\n",
      "Processed:  124\n",
      "Processed:  125\n",
      "Processed:  126\n",
      "Processed:  127\n",
      "Processed:  128\n",
      "Processed:  129\n",
      "Processed:  130\n",
      "Processed:  131\n",
      "Processed:  132\n",
      "Processed:  133\n",
      "Processed:  134\n",
      "Processed:  135\n",
      "Processed:  136\n",
      "Processed:  137\n",
      "Processed:  138\n",
      "Processed:  139\n",
      "Processed:  140\n",
      "Processed:  141\n",
      "Processed:  142\n",
      "Processed:  143\n",
      "Processed:  144\n",
      "Processed:  145\n",
      "Processed:  146\n",
      "Processed:  147\n",
      "Processed:  148\n",
      "Processed:  149\n",
      "Processed:  150\n",
      "Processed:  151\n",
      "Processed:  152\n",
      "Processed:  153\n",
      "Processed:  154\n",
      "Processed:  155\n",
      "Processed:  156\n",
      "Processed:  157\n",
      "Processed:  158\n",
      "Processed:  159\n",
      "Processed:  160\n",
      "Processed:  161\n",
      "Processed:  162\n",
      "Processed:  163\n",
      "Processed:  164\n",
      "Processed:  165\n",
      "Processed:  166\n",
      "Processed:  167\n",
      "Processed:  168\n",
      "Processed:  169\n",
      "Processed:  170\n",
      "Processed:  171\n",
      "Processed:  172\n",
      "Processed:  173\n",
      "Processed:  174\n",
      "Processed:  175\n",
      "Processed:  176\n",
      "Processed:  177\n",
      "Processed:  178\n",
      "Processed:  179\n",
      "Processed:  180\n",
      "Processed:  181\n",
      "Processed:  182\n",
      "Processed:  183\n",
      "Processed:  184\n",
      "Processed:  185\n",
      "Processed:  186\n",
      "Processed:  187\n",
      "Processed:  188\n",
      "Processed:  189\n",
      "Processed:  190\n",
      "Processed:  191\n",
      "Processed:  192\n",
      "Processed:  193\n",
      "Processed:  194\n",
      "Processed:  195\n",
      "Processed:  196\n",
      "Processed:  197\n",
      "Processed:  198\n",
      "Processed:  199\n",
      "Processed:  200\n",
      "Processed:  201\n",
      "Processed:  202\n",
      "Processed:  203\n",
      "Processed:  204\n",
      "Processed:  205\n",
      "Processed:  206\n",
      "Processed:  207\n",
      "Processed:  208\n",
      "Processed:  209\n",
      "Processed:  210\n",
      "Processed:  211\n",
      "Processed:  212\n",
      "Processed:  213\n",
      "Processed:  214\n",
      "Processed:  215\n",
      "Processed:  216\n",
      "Processed:  217\n",
      "Processed:  218\n",
      "Processed:  219\n",
      "Processed:  220\n",
      "Processed:  221\n",
      "Processed:  222\n",
      "Processed:  223\n",
      "Processed:  224\n",
      "Processed:  225\n",
      "Processed:  226\n",
      "Processed:  227\n",
      "Processed:  228\n",
      "Processed:  229\n",
      "Processed:  230\n",
      "Processed:  231\n",
      "Processed:  232\n",
      "Processed:  233\n",
      "Processed:  234\n",
      "Processed:  235\n",
      "Processed:  236\n",
      "Processed:  237\n",
      "Processed:  238\n",
      "Processed:  239\n",
      "Processed:  240\n",
      "Processed:  241\n",
      "Processed:  242\n",
      "Processed:  243\n",
      "Processed:  244\n",
      "Processed:  245\n",
      "Processed:  246\n",
      "Processed:  247\n",
      "Processed:  248\n",
      "Processed:  249\n",
      "Processed:  250\n",
      "Processed:  251\n",
      "Processed:  252\n",
      "Processed:  253\n",
      "Processed:  254\n",
      "Processed:  255\n",
      "Processed:  256\n",
      "Processed:  257\n",
      "Processed:  258\n",
      "Processed:  259\n",
      "Processed:  260\n",
      "Processed:  261\n",
      "Processed:  262\n",
      "Processed:  263\n",
      "Processed:  264\n",
      "Processed:  265\n",
      "Finalizing...\n"
     ]
    }
   ],
   "source": [
    "counts, means, M2s = np.zeros(19), np.zeros(19), np.zeros(19)\n",
    "final_means, final_stds = np.zeros(19), np.zeros(19)\n",
    "\n",
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for i, file in enumerate(files_builder(DataKind('processed'))):\n",
    "    compute_sep(file, DIRECT_ROOT)\n",
    "    print('Processed: ', i)\n",
    "    \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "print('Saving...')\n",
    "for fn in os.listdir(DIRECT_ROOT):\n",
    "    filepath = os.path.join(DIRECT_ROOT,fn)\n",
    "    r = np.load(filepath)\n",
    "    assert r.shape[0] == len(CHANNEL_NAMES)\n",
    "    for i in np.arange(r.shape[0]):\n",
    "        r[i, :] = (r[i, :]-final_means[i])/final_stds[i]\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_middle(filenames, labels):\n",
    "    ta = zip(filenames, labels)\n",
    "    ta = np.array([(t, l) for t, l in ta], dtype=[('fname', 'S100'), ('label', 'int64')])\n",
    "    ta = ta[:][(ta['label'] == -1) | (ta['label'] == 1)]\n",
    "    ta['label'][ta['label'] == -1] = 0\n",
    "    return ta['fname'].astype(str, copy=False), ta['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distribution:  {0: 6801, 1: 6006}\n",
      "Training distribution:  {0: 4718, 1: 4246}\n",
      "Testing distribution:  {0: 1027, 1: 894}\n",
      "Validation distribution:  {0: 1056, 1: 866}\n"
     ]
    }
   ],
   "source": [
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "from lib.braindecode.braindecode.datautil.splitters import split_into_two_sets\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "fb = files_builder(DataKind('recplot'), subfolder=('vectors',))\n",
    "seed = 123\n",
    "fns = [fn[1] for fn in fb.file_names(include_path=True, subfolder=(), recursive=True)]\n",
    "filenames, labels = remove_middle(fns, fb.get_labels(fns))\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print('Overall distribution: ', dict(zip(unique, counts)))\n",
    "training_filenames, validation_filenames, training_labels, validation_labels = \\\n",
    "    train_test_split(filenames, labels, test_size=0.3, random_state=seed)\n",
    "test_filenames, validation_filenames, test_labels, validation_labels = \\\n",
    "    train_test_split(validation_filenames, validation_labels, test_size=0.5, random_state=seed)\n",
    "assert len(training_filenames) == len(training_labels)\n",
    "assert len(validation_filenames) == len(validation_labels)\n",
    "unique, counts = np.unique(training_labels, return_counts=True)\n",
    "print('Training distribution: ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(test_labels, return_counts=True)\n",
    "print('Testing distribution: ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "print('Validation distribution: ', dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.array([np.load(os.path.join(REC_ROOT, fn)).squeeze().astype('float32') for fn in training_filenames])\n",
    "test_set = np.array([np.load(os.path.join(REC_ROOT, fn)).squeeze().astype('float32') for fn in test_filenames])\n",
    "valid_set = np.array([np.load(os.path.join(REC_ROOT, fn)).squeeze().astype('float32') for fn in validation_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.signal_target import SignalAndTarget\n",
    "\n",
    "train_set = SignalAndTarget(train_set, y=training_labels)\n",
    "valid_set = SignalAndTarget(valid_set, y=validation_labels)\n",
    "test_set = SignalAndTarget(test_set, y=test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model, optimization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chans = 256\n",
    "n_classes = 2\n",
    "cuda = False\n",
    "input_time_length = 256\n",
    "n_epochs = 35\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      "Sequential(\n",
      "  (dimshuffle): Expression(expression=_transpose_time_to_spat)\n",
      "  (conv_time): Conv2d(1, 40, kernel_size=(25, 1), stride=(1, 1))\n",
      "  (conv_spat): Conv2d(40, 40, kernel_size=(1, 256), stride=(1, 1), bias=False)\n",
      "  (bnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_nonlin): Expression(expression=square)\n",
      "  (pool): AvgPool2d(kernel_size=(75, 1), stride=(15, 1), padding=0)\n",
      "  (pool_nonlin): Expression(expression=safe_log)\n",
      "  (drop): Dropout(p=0.5)\n",
      "  (conv_classifier): Conv2d(40, 2, kernel_size=(11, 1), stride=(1, 1))\n",
      "  (softmax): LogSoftmax()\n",
      "  (squeeze): Expression(expression=_squeeze_final_output)\n",
      ") \n",
      "Number of params:  411602\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.functions import safe_log, square\n",
    "# log = logging.getLogger(__name__)\n",
    "model = ShallowFBCSPNet(\n",
    "    n_chans, n_classes, input_time_length=input_time_length,\n",
    "    n_filters_time=40,\n",
    "    filter_time_length=25,\n",
    "    n_filters_spat=40,\n",
    "    pool_time_length=75,\n",
    "    pool_time_stride=15,\n",
    "    final_conv_length='auto',\n",
    "    conv_nonlin=square,\n",
    "    pool_mode='mean',\n",
    "    pool_nonlin=safe_log,\n",
    "    split_first_layer=True,\n",
    "    batch_norm=True,\n",
    "    batch_norm_alpha=0.1,\n",
    "    drop_prob=0.5,\n",
    ").create_network()\n",
    "# model = Deep4Net(n_chans, n_classes, input_time_length=input_time_length, final_conv_length=20).create_network()\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "print(\"Model: \\n{:s}\".format(str(model)), '\\nNumber of params: ', sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from braindecode.torch_ext.schedulers import ScheduledOptimizer, CosineAnnealing\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState((2018,8,7))\n",
    "# optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)\n",
    "# Need to determine number of batch passes per epoch for cosine annealing\n",
    "n_updates_per_epoch = len(list(get_balanced_batches(len(train_set.X), rng, shuffle=True,\n",
    "                                            batch_size=batch_size)))\n",
    "scheduler = CosineAnnealing(n_epochs * n_updates_per_epoch)\n",
    "# schedule_weight_decay must be True for AdamW\n",
    "optimizer = ScheduledOptimizer(scheduler, optimizer, schedule_weight_decay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual trial-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 79GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544199946412/work/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7c5b4ee40917>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mnet_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         print(\"{:6s} Loss: {:.5f}\".format(\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 79GB. Buy new RAM! at /opt/conda/conda-bld/pytorch_1544199946412/work/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "for i_epoch in range(n_epochs):\n",
    "    i_trials_in_batch = get_balanced_batches(len(train_set.X), rng, shuffle=True,\n",
    "                                            batch_size=batch_size)\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for i_trials in i_trials_in_batch:\n",
    "        # Have to add empty fourth dimension to X\n",
    "        batch_X = train_set.X[i_trials][:,:,:,None]\n",
    "        batch_y = train_set.y[i_trials]\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Compute outputs of the network\n",
    "        outputs = model(net_in)\n",
    "        # Compute the loss\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        # Do the backpropagation\n",
    "        loss.backward()\n",
    "        # Update parameters with the optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set), ('Valid', valid_set)):\n",
    "        # Here, we will use the entire dataset at once, which is still possible\n",
    "        # for such smaller datasets. Otherwise we would have to use batches.\n",
    "        net_in = np_to_var(dataset.X[:,:,:,None])\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(dataset.y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        outputs = model(net_in)\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(\n",
    "            setname, float(var_to_np(loss))))\n",
    "        predicted_labels = np.argmax(var_to_np(outputs), axis=1)\n",
    "        accuracy = np.mean(dataset.y  == predicted_labels)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2 or more dimensions (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-6848208a8e9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1782\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1784\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2 or more dimensions (got 1)"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "model.eval()\n",
    "# Collect all predictions and losses\n",
    "all_preds = []\n",
    "all_losses = []\n",
    "batch_sizes = []\n",
    "for batch_X, batch_y in iterator.get_batches(test_set, shuffle=False):\n",
    "    net_in = np_to_var(batch_X)\n",
    "    if cuda:\n",
    "        net_in = net_in.cuda()\n",
    "    net_target = np_to_var(batch_y)\n",
    "    if cuda:\n",
    "        net_target = net_target.cuda()\n",
    "    outputs = model(net_in)\n",
    "    all_preds.append(var_to_np(outputs))\n",
    "    loss = F.nll_loss(outputs, net_target)\n",
    "    loss = float(var_to_np(loss))\n",
    "    all_losses.append(loss)\n",
    "    batch_sizes.append(len(batch_X))\n",
    "# Compute mean per-input loss\n",
    "loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "               np.mean(batch_sizes))\n",
    "print(\"Test Loss: {:.5f}\".format(loss))\n",
    "# Assign the predictions to the trials\n",
    "preds_per_trial = compute_preds_per_trial_from_crops(all_preds,\n",
    "                                                  input_time_length,\n",
    "                                                  test_set.X)\n",
    "# preds per trial are now trials x classes x timesteps/predictions\n",
    "# Now mean across timesteps for each trial to get per-trial predictions\n",
    "meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "accuracy = np.mean(predicted_labels == test_set.y)\n",
    "print(\"Test Accuracy: {:.1f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic trial-wise training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 35\n",
    "max_increase_epochs = 4\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "iterator = BalancedBatchSizeIterator(batch_size=batch_size)\n",
    "\n",
    "stop_criterion = Or([MaxEpochs(max_epochs),\n",
    "                     NoDecrease('valid_misclass', max_increase_epochs)])\n",
    "\n",
    "monitors = [LossMonitor(), MisclassMonitor(), RuntimeMonitor()]\n",
    "\n",
    "model_constraint = MaxNormDefaultConstraint()\n",
    "\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator=iterator,\n",
    "                 loss_function=F.nll_loss, optimizer=optimizer,\n",
    "                 model_constraint=model_constraint,\n",
    "                 monitors=monitors,\n",
    "                 stop_criterion=stop_criterion,\n",
    "                 remember_best_column='valid_misclass',\n",
    "                 run_after_early_stop=True, cuda=cuda)\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 10 epochs\n",
      "\n",
      "    train_loss  valid_loss  test_loss  train_misclass  valid_misclass  \\\n",
      "45    0.313250    0.336083   0.372132        0.114367        0.127992   \n",
      "46    0.295478    0.312901   0.361784        0.103252        0.119147   \n",
      "47    0.342636    0.348469   0.419809        0.148815        0.146722   \n",
      "48    0.309392    0.315402   0.381417        0.127503        0.126431   \n",
      "49    0.312134    0.324736   0.381960        0.122818        0.129553   \n",
      "50    0.318551    0.328828   0.376518        0.123186        0.134755   \n",
      "51    0.291159    0.306112   0.357053        0.094984        0.106139   \n",
      "52    0.295919    0.306773   0.359336        0.108580        0.119667   \n",
      "53    0.298605    0.306146   0.363143        0.112071        0.113944   \n",
      "54    0.291833    0.295663   0.357623        0.101415        0.106139   \n",
      "\n",
      "    test_misclass    runtime  \n",
      "45       0.161374  17.108581  \n",
      "46       0.152004  17.179595  \n",
      "47       0.187402  17.236296  \n",
      "48       0.176471  17.242505  \n",
      "49       0.161374  17.051704  \n",
      "50       0.161374  17.087500  \n",
      "51       0.147319  17.113561  \n",
      "52       0.146278  17.399459  \n",
      "53       0.160854  17.380983  \n",
      "54       0.145757  17.162923  \n"
     ]
    }
   ],
   "source": [
    "print(\"Last 10 epochs\")\n",
    "print(\"\\n\" + str(exp.epochs_df.iloc[-10:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual cropped training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_chans = 19\n",
    "n_classes = 2\n",
    "cuda = False\n",
    "input_time_length = 256\n",
    "n_epochs = 50\n",
    "max_epochs = n_epochs\n",
    "max_increase_epochs = 4\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.models.shallow_fbcsp import ShallowFBCSPNet\n",
    "from torch import nn\n",
    "from braindecode.torch_ext.util import set_random_seeds\n",
    "from braindecode.models.util import to_dense_prediction_model\n",
    "\n",
    "# Set if you want to use GPU\n",
    "# You can also use torch.cuda.is_available() to determine if cuda is available on your machine.\n",
    "set_random_seeds(seed=20170629, cuda=cuda)\n",
    "\n",
    "# This will determine how many crops are processed in parallel\n",
    "n_classes = 2\n",
    "in_chans = train_set.X.shape[1]\n",
    "# final_conv_length determines the size of the receptive field of the ConvNet\n",
    "model = ShallowFBCSPNet(in_chans=in_chans, n_classes=n_classes, input_time_length=input_time_length,\n",
    "                        final_conv_length='auto').create_network()\n",
    "to_dense_prediction_model(model)\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 predictions per input/trial\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var\n",
    "# determine output size\n",
    "# test_input = np_to_var(np.ones((2, in_chans, input_time_length, 1), dtype=np.float32))\n",
    "test_input = np_to_var(train_set.X[:1, :, :, None])\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "out = model(test_input)\n",
    "n_preds_per_input = out.cpu().data.numpy().shape[2]\n",
    "\n",
    "print(\"{:d} predictions per input/trial\".format(n_preds_per_input))\n",
    "\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size,input_time_length=input_time_length,\n",
    "                                  n_preds_per_input=n_preds_per_input)\n",
    "\n",
    "from braindecode.torch_ext.optimizers import AdamW\n",
    "from braindecode.torch_ext.schedulers import ScheduledOptimizer, CosineAnnealing\n",
    "from braindecode.datautil.iterators import get_balanced_batches\n",
    "from numpy.random import RandomState\n",
    "rng = RandomState((2018,8,7))\n",
    "#optimizer = AdamW(model.parameters(), lr=1*0.01, weight_decay=0.5*0.001) # these are good values for the deep model\n",
    "optimizer = AdamW(model.parameters(), lr=0.0625 * 0.01, weight_decay=0)\n",
    "# Need to determine number of batch passes per epoch for cosine annealing\n",
    "n_updates_per_epoch = len([None for b in iterator.get_batches(train_set, True)])\n",
    "scheduler = CosineAnnealing(n_epochs * n_updates_per_epoch)\n",
    "# schedule_weight_decay must be True for AdamW\n",
    "optimizer = ScheduledOptimizer(scheduler, optimizer, schedule_weight_decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Train  Loss: 0.58090\n",
      "Train  Accuracy: 69.3%\n",
      "Valid  Loss: 0.66640\n",
      "Valid  Accuracy: 61.9%\n",
      "Epoch 1\n",
      "Train  Loss: 0.50311\n",
      "Train  Accuracy: 76.6%\n",
      "Valid  Loss: 0.60199\n",
      "Valid  Accuracy: 68.5%\n",
      "Epoch 2\n",
      "Train  Loss: 0.48061\n",
      "Train  Accuracy: 77.1%\n",
      "Valid  Loss: 0.60609\n",
      "Valid  Accuracy: 68.3%\n",
      "Epoch 3\n",
      "Train  Loss: 0.42770\n",
      "Train  Accuracy: 81.1%\n",
      "Valid  Loss: 0.55219\n",
      "Valid  Accuracy: 71.6%\n",
      "Epoch 4\n",
      "Train  Loss: 0.38405\n",
      "Train  Accuracy: 83.8%\n",
      "Valid  Loss: 0.52567\n",
      "Valid  Accuracy: 76.1%\n",
      "Epoch 5\n",
      "Train  Loss: 0.34461\n",
      "Train  Accuracy: 86.3%\n",
      "Valid  Loss: 0.50529\n",
      "Valid  Accuracy: 73.6%\n",
      "Epoch 6\n",
      "Train  Loss: 0.32876\n",
      "Train  Accuracy: 86.8%\n",
      "Valid  Loss: 0.47941\n",
      "Valid  Accuracy: 77.5%\n",
      "Epoch 7\n",
      "Train  Loss: 0.36207\n",
      "Train  Accuracy: 83.1%\n",
      "Valid  Loss: 0.54554\n",
      "Valid  Accuracy: 73.7%\n",
      "Epoch 8\n",
      "Train  Loss: 0.30392\n",
      "Train  Accuracy: 87.5%\n",
      "Valid  Loss: 0.47964\n",
      "Valid  Accuracy: 77.3%\n",
      "Epoch 9\n",
      "Train  Loss: 0.25175\n",
      "Train  Accuracy: 91.2%\n",
      "Valid  Loss: 0.44010\n",
      "Valid  Accuracy: 78.3%\n",
      "Epoch 10\n",
      "Train  Loss: 0.26492\n",
      "Train  Accuracy: 89.2%\n",
      "Valid  Loss: 0.47374\n",
      "Valid  Accuracy: 77.6%\n",
      "Epoch 11\n",
      "Train  Loss: 0.22370\n",
      "Train  Accuracy: 92.4%\n",
      "Valid  Loss: 0.42841\n",
      "Valid  Accuracy: 80.5%\n",
      "Epoch 12\n",
      "Train  Loss: 0.20980\n",
      "Train  Accuracy: 93.3%\n",
      "Valid  Loss: 0.41639\n",
      "Valid  Accuracy: 80.4%\n",
      "Epoch 13\n",
      "Train  Loss: 0.21790\n",
      "Train  Accuracy: 92.7%\n",
      "Valid  Loss: 0.44232\n",
      "Valid  Accuracy: 79.0%\n",
      "Epoch 14\n",
      "Train  Loss: 0.20886\n",
      "Train  Accuracy: 92.8%\n",
      "Valid  Loss: 0.44082\n",
      "Valid  Accuracy: 79.6%\n",
      "Epoch 15\n",
      "Train  Loss: 0.18294\n",
      "Train  Accuracy: 94.7%\n",
      "Valid  Loss: 0.39853\n",
      "Valid  Accuracy: 82.5%\n",
      "Epoch 16\n",
      "Train  Loss: 0.17592\n",
      "Train  Accuracy: 95.5%\n",
      "Valid  Loss: 0.41185\n",
      "Valid  Accuracy: 82.3%\n",
      "Epoch 17\n",
      "Train  Loss: 0.16456\n",
      "Train  Accuracy: 95.3%\n",
      "Valid  Loss: 0.39632\n",
      "Valid  Accuracy: 82.3%\n",
      "Epoch 18\n",
      "Train  Loss: 0.18438\n",
      "Train  Accuracy: 93.4%\n",
      "Valid  Loss: 0.43011\n",
      "Valid  Accuracy: 80.5%\n",
      "Epoch 19\n",
      "Train  Loss: 0.14156\n",
      "Train  Accuracy: 95.9%\n",
      "Valid  Loss: 0.38490\n",
      "Valid  Accuracy: 84.3%\n",
      "Epoch 20\n",
      "Train  Loss: 0.15062\n",
      "Train  Accuracy: 95.7%\n",
      "Valid  Loss: 0.41755\n",
      "Valid  Accuracy: 81.9%\n",
      "Epoch 21\n",
      "Train  Loss: 0.12703\n",
      "Train  Accuracy: 96.7%\n",
      "Valid  Loss: 0.37481\n",
      "Valid  Accuracy: 83.6%\n",
      "Epoch 22\n",
      "Train  Loss: 0.12022\n",
      "Train  Accuracy: 97.2%\n",
      "Valid  Loss: 0.36396\n",
      "Valid  Accuracy: 84.0%\n",
      "Epoch 23\n",
      "Train  Loss: 0.11235\n",
      "Train  Accuracy: 97.6%\n",
      "Valid  Loss: 0.36917\n",
      "Valid  Accuracy: 84.5%\n",
      "Epoch 24\n",
      "Train  Loss: 0.12001\n",
      "Train  Accuracy: 96.9%\n",
      "Valid  Loss: 0.38299\n",
      "Valid  Accuracy: 83.4%\n",
      "Epoch 25\n",
      "Train  Loss: 0.10371\n",
      "Train  Accuracy: 97.9%\n",
      "Valid  Loss: 0.37611\n",
      "Valid  Accuracy: 84.0%\n",
      "Epoch 26\n",
      "Train  Loss: 0.11742\n",
      "Train  Accuracy: 96.5%\n",
      "Valid  Loss: 0.39179\n",
      "Valid  Accuracy: 83.1%\n",
      "Epoch 27\n",
      "Train  Loss: 0.08701\n",
      "Train  Accuracy: 98.6%\n",
      "Valid  Loss: 0.34069\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 28\n",
      "Train  Loss: 0.08775\n",
      "Train  Accuracy: 98.5%\n",
      "Valid  Loss: 0.36553\n",
      "Valid  Accuracy: 84.7%\n",
      "Epoch 29\n",
      "Train  Loss: 0.09773\n",
      "Train  Accuracy: 97.8%\n",
      "Valid  Loss: 0.38627\n",
      "Valid  Accuracy: 84.5%\n",
      "Epoch 30\n",
      "Train  Loss: 0.08489\n",
      "Train  Accuracy: 98.6%\n",
      "Valid  Loss: 0.36258\n",
      "Valid  Accuracy: 84.6%\n",
      "Epoch 31\n",
      "Train  Loss: 0.07946\n",
      "Train  Accuracy: 98.8%\n",
      "Valid  Loss: 0.35441\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 32\n",
      "Train  Loss: 0.07389\n",
      "Train  Accuracy: 99.0%\n",
      "Valid  Loss: 0.35158\n",
      "Valid  Accuracy: 85.4%\n",
      "Epoch 33\n",
      "Train  Loss: 0.07610\n",
      "Train  Accuracy: 98.8%\n",
      "Valid  Loss: 0.35245\n",
      "Valid  Accuracy: 85.4%\n",
      "Epoch 34\n",
      "Train  Loss: 0.07276\n",
      "Train  Accuracy: 99.0%\n",
      "Valid  Loss: 0.36068\n",
      "Valid  Accuracy: 85.3%\n",
      "Epoch 35\n",
      "Train  Loss: 0.06980\n",
      "Train  Accuracy: 99.2%\n",
      "Valid  Loss: 0.35208\n",
      "Valid  Accuracy: 85.4%\n",
      "Epoch 36\n",
      "Train  Loss: 0.06552\n",
      "Train  Accuracy: 99.2%\n",
      "Valid  Loss: 0.34967\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 37\n",
      "Train  Loss: 0.06544\n",
      "Train  Accuracy: 99.2%\n",
      "Valid  Loss: 0.34674\n",
      "Valid  Accuracy: 86.5%\n",
      "Epoch 38\n",
      "Train  Loss: 0.06439\n",
      "Train  Accuracy: 99.2%\n",
      "Valid  Loss: 0.35494\n",
      "Valid  Accuracy: 85.4%\n",
      "Epoch 39\n",
      "Train  Loss: 0.06216\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34704\n",
      "Valid  Accuracy: 85.7%\n",
      "Epoch 40\n",
      "Train  Loss: 0.06231\n",
      "Train  Accuracy: 99.3%\n",
      "Valid  Loss: 0.35105\n",
      "Valid  Accuracy: 85.5%\n",
      "Epoch 41\n",
      "Train  Loss: 0.06331\n",
      "Train  Accuracy: 99.3%\n",
      "Valid  Loss: 0.35379\n",
      "Valid  Accuracy: 85.1%\n",
      "Epoch 42\n",
      "Train  Loss: 0.06178\n",
      "Train  Accuracy: 99.3%\n",
      "Valid  Loss: 0.34912\n",
      "Valid  Accuracy: 86.1%\n",
      "Epoch 43\n",
      "Train  Loss: 0.06314\n",
      "Train  Accuracy: 99.3%\n",
      "Valid  Loss: 0.35408\n",
      "Valid  Accuracy: 85.3%\n",
      "Epoch 44\n",
      "Train  Loss: 0.05916\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34550\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 45\n",
      "Train  Loss: 0.05911\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34634\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 46\n",
      "Train  Loss: 0.05958\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34588\n",
      "Valid  Accuracy: 85.9%\n",
      "Epoch 47\n",
      "Train  Loss: 0.05870\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34549\n",
      "Valid  Accuracy: 85.9%\n",
      "Epoch 48\n",
      "Train  Loss: 0.05879\n",
      "Train  Accuracy: 99.4%\n",
      "Valid  Loss: 0.34573\n",
      "Valid  Accuracy: 85.8%\n",
      "Epoch 49\n",
      "Train  Loss: 0.05911\n",
      "Train  Accuracy: 99.5%\n",
      "Valid  Loss: 0.34655\n",
      "Valid  Accuracy: 85.4%\n"
     ]
    }
   ],
   "source": [
    "from braindecode.torch_ext.util import np_to_var, var_to_np\n",
    "import torch.nn.functional as F\n",
    "from numpy.random import RandomState\n",
    "import torch as th\n",
    "from braindecode.experiments.monitors import compute_preds_per_trial_from_crops\n",
    "rng = RandomState((2017,6,30))\n",
    "for i_epoch in range(n_epochs):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    for batch_X, batch_y in iterator.get_batches(train_set, shuffle=True):\n",
    "        net_in = np_to_var(batch_X)\n",
    "        if cuda:\n",
    "            net_in = net_in.cuda()\n",
    "        net_target = np_to_var(batch_y)\n",
    "        if cuda:\n",
    "            net_target = net_target.cuda()\n",
    "        # Remove gradients of last backward pass from all parameters\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(net_in)\n",
    "        # Mean predictions across trial\n",
    "        # Note that this will give identical gradients to computing\n",
    "        # a per-prediction loss (at least for the combination of log softmax activation\n",
    "        # and negative log likelihood loss which we are using here)\n",
    "        outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "        loss = F.nll_loss(outputs, net_target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print some statistics each epoch\n",
    "    model.eval()\n",
    "    print(\"Epoch {:d}\".format(i_epoch))\n",
    "    for setname, dataset in (('Train', train_set),('Valid', valid_set)):\n",
    "        # Collect all predictions and losses\n",
    "        all_preds = []\n",
    "        all_losses = []\n",
    "        batch_sizes = []\n",
    "        for batch_X, batch_y in iterator.get_batches(dataset, shuffle=False):\n",
    "            net_in = np_to_var(batch_X)\n",
    "            if cuda:\n",
    "                net_in = net_in.cuda()\n",
    "            net_target = np_to_var(batch_y)\n",
    "            if cuda:\n",
    "                net_target = net_target.cuda()\n",
    "            outputs = model(net_in)\n",
    "            all_preds.append(var_to_np(outputs))\n",
    "            outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "            loss = F.nll_loss(outputs, net_target)\n",
    "            loss = float(var_to_np(loss))\n",
    "            all_losses.append(loss)\n",
    "            batch_sizes.append(len(batch_X))\n",
    "        # Compute mean per-input loss\n",
    "        loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "                       np.mean(batch_sizes))\n",
    "        print(\"{:6s} Loss: {:.5f}\".format(setname, loss))\n",
    "        # Assign the predictions to the trials\n",
    "        preds_per_trial = compute_preds_per_trial_from_crops(all_preds,\n",
    "                                                          input_time_length,\n",
    "                                                          dataset.X)\n",
    "        # preds per trial are now trials x classes x timesteps/predictions\n",
    "        # Now mean across timesteps for each trial to get per-trial predictions\n",
    "        meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "        predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "        accuracy = np.mean(predicted_labels == dataset.y)\n",
    "        print(\"{:6s} Accuracy: {:.1f}%\".format(\n",
    "            setname, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.40381\n",
      "Test Accuracy: 84.4%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Collect all predictions and losses\n",
    "all_preds = []\n",
    "all_losses = []\n",
    "batch_sizes = []\n",
    "for batch_X, batch_y in iterator.get_batches(test_set, shuffle=False):\n",
    "    net_in = np_to_var(batch_X)\n",
    "    if cuda:\n",
    "        net_in = net_in.cuda()\n",
    "    net_target = np_to_var(batch_y)\n",
    "    if cuda:\n",
    "        net_target = net_target.cuda()\n",
    "    outputs = model(net_in)\n",
    "    all_preds.append(var_to_np(outputs))\n",
    "    outputs = th.mean(outputs, dim=2, keepdim=False)\n",
    "    loss = F.nll_loss(outputs, net_target)\n",
    "    loss = float(var_to_np(loss))\n",
    "    all_losses.append(loss)\n",
    "    batch_sizes.append(len(batch_X))\n",
    "# Compute mean per-input loss\n",
    "loss = np.mean(np.array(all_losses) * np.array(batch_sizes) /\n",
    "               np.mean(batch_sizes))\n",
    "print(\"Test Loss: {:.5f}\".format(loss))\n",
    "# Assign the predictions to the trials\n",
    "preds_per_trial = compute_preds_per_trial_from_crops(all_preds,\n",
    "                                                  input_time_length,\n",
    "                                                  test_set.X)\n",
    "# preds per trial are now trials x classes x timesteps/predictions\n",
    "# Now mean across timesteps for each trial to get per-trial predictions\n",
    "meaned_preds_per_trial = np.array([np.mean(p, axis=1) for p in preds_per_trial])\n",
    "predicted_labels = np.argmax(meaned_preds_per_trial, axis=1)\n",
    "accuracy = np.mean(predicted_labels == test_set.y)\n",
    "print(\"Test Accuracy: {:.1f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic cropped training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.experiments.monitors import LossMonitor, MisclassMonitor, RuntimeMonitor, CroppedTrialMisclassMonitor\n",
    "from braindecode.datautil.iterators import CropsFromTrialsIterator\n",
    "from braindecode.torch_ext.constraints import MaxNormDefaultConstraint\n",
    "from braindecode.experiments.stopcriteria import MaxEpochs, NoDecrease, Or\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "iterator = CropsFromTrialsIterator(batch_size=batch_size,\n",
    "                                   input_time_length=input_time_length,\n",
    "                                   n_preds_per_input=n_preds_per_input)\n",
    "stop_criterion = Or([MaxEpochs(max_epochs),\n",
    "                     NoDecrease('valid_misclass', max_increase_epochs)])\n",
    "monitors = [LossMonitor(), MisclassMonitor(col_suffix='sample_misclass'),\n",
    "            CroppedTrialMisclassMonitor(\n",
    "                input_time_length=input_time_length), RuntimeMonitor()]\n",
    "\n",
    "model_constraint = MaxNormDefaultConstraint()\n",
    "\n",
    "loss_function = lambda preds, targets: F.nll_loss(\n",
    "    th.mean(preds, dim=2, keepdim=False), targets)\n",
    "\n",
    "exp = Experiment(model, train_set, valid_set, test_set, iterator=iterator,\n",
    "                 loss_function=loss_function, optimizer=optimizer,\n",
    "                 model_constraint=model_constraint,\n",
    "                 monitors=monitors,\n",
    "                 stop_criterion=stop_criterion,\n",
    "                 remember_best_column='valid_misclass',\n",
    "                 run_after_early_stop=True, cuda=cuda)\n",
    "exp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 20 epochs\n",
      "\n",
      "    train_loss  valid_loss  test_loss  train_sample_misclass  \\\n",
      "10    0.347806    0.437950   0.421939               0.140590   \n",
      "11    0.335104    0.433781   0.419113               0.131791   \n",
      "12    0.330339    0.434856   0.408854               0.130982   \n",
      "13    0.355878    0.456078   0.442913               0.156250   \n",
      "14    0.332419    0.445152   0.412524               0.130913   \n",
      "15    0.315801    0.426716   0.399017               0.115615   \n",
      "16    0.313600    0.428834   0.402357               0.120203   \n",
      "17    0.414201    0.520785   0.523055               0.188351   \n",
      "18    0.311591    0.411437   0.396985               0.117693   \n",
      "19    0.329220    0.411437   0.396985               0.129915   \n",
      "20    0.315685    0.371553   0.382071               0.115860   \n",
      "21    0.328018    0.374032   0.392980               0.128422   \n",
      "22    0.319208    0.359238   0.390692               0.122129   \n",
      "23    0.318259    0.360736   0.386656               0.121969   \n",
      "24    0.332172    0.373710   0.399549               0.134657   \n",
      "25    0.331901    0.349313   0.408632               0.141971   \n",
      "26    0.319345    0.348859   0.392084               0.125632   \n",
      "27    0.293106    0.318471   0.362479               0.105755   \n",
      "28    0.314480    0.331354   0.396914               0.122738   \n",
      "29    0.284529    0.307821   0.351319               0.093939   \n",
      "\n",
      "    valid_sample_misclass  test_sample_misclass  train_misclass  \\\n",
      "10               0.201938              0.200547        0.140451   \n",
      "11               0.196930              0.193194        0.132084   \n",
      "12               0.200963              0.187467        0.130522   \n",
      "13               0.214555              0.201783        0.156292   \n",
      "14               0.206230              0.187402        0.130411   \n",
      "15               0.192443              0.183824        0.115797   \n",
      "16               0.193483              0.174909        0.119924   \n",
      "17               0.234391              0.239133        0.187863   \n",
      "18               0.186915              0.174584        0.117135   \n",
      "19               0.186915              0.174584        0.129616   \n",
      "20               0.154201              0.163522        0.115653   \n",
      "21               0.162591              0.176210        0.128330   \n",
      "22               0.155372              0.182847        0.121349   \n",
      "23               0.152836              0.161374        0.121440   \n",
      "24               0.163176              0.190135        0.134944   \n",
      "25               0.157713              0.188378        0.142017   \n",
      "26               0.149194              0.169898        0.125482   \n",
      "27               0.126691              0.152980        0.106008   \n",
      "28               0.138788              0.174258        0.123002   \n",
      "29               0.105684              0.152199        0.094249   \n",
      "\n",
      "    valid_misclass  test_misclass    runtime  \n",
      "10        0.200832       0.199896  29.582859  \n",
      "11        0.197711       0.192087  29.469643  \n",
      "12        0.199272       0.188444  29.953539  \n",
      "13        0.212279       0.201458  29.999656  \n",
      "14        0.207076       0.187402  30.158527  \n",
      "15        0.192508       0.182717  30.183896  \n",
      "16        0.193028       0.173347  29.452544  \n",
      "17        0.234651       0.237897  29.445220  \n",
      "18        0.187825       0.175950  29.417546  \n",
      "19        0.187825       0.175950   9.471253  \n",
      "20        0.154527       0.163457  35.431058  \n",
      "21        0.162851       0.176471  35.301726  \n",
      "22        0.155567       0.183238  35.149961  \n",
      "23        0.151405       0.158771  35.149463  \n",
      "24        0.163371       0.192087  35.865864  \n",
      "25        0.158169       0.190005  35.450397  \n",
      "26        0.149324       0.168662  35.158763  \n",
      "27        0.126431       0.152525  35.121161  \n",
      "28        0.139438       0.172306  35.095753  \n",
      "29        0.106139       0.152004  35.170330  \n"
     ]
    }
   ],
   "source": [
    "print(\"Last 20 epochs\")\n",
    "print(\"\\n\" + str(exp.epochs_df.iloc[-20:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
