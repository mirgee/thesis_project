{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Thu Jan 24 2019 \n",
      "\n",
      "CPython 3.6.6\n",
      "IPython 6.5.0\n",
      "\n",
      "numpy 1.15.1\n",
      "scipy 1.1.0\n",
      "sklearn 0.19.1\n",
      "pandas 0.23.4\n",
      "\n",
      "compiler   : GCC 4.8.2 20140120 (Red Hat 4.8.2-15)\n",
      "system     : Linux\n",
      "release    : 4.20.3-arch1-1-ARCH\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kovar/thesis_project/\n",
      "/home/kovar/thesis_project/data\n",
      "/home/kovar/thesis_project/data/processed\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nolds\n",
    "import data\n",
    "import mne\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "\n",
    "PROJ_ROOT = os.getenv('THESIS_ROOT')\n",
    "DATA_ROOT = os.path.abspath(os.path.join(PROJ_ROOT, 'data'))\n",
    "PROCESSED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'processed'))\n",
    "RAW_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'raw'))\n",
    "LABELED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'labeled'))\n",
    "DURATIONS_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'durations'))\n",
    "REC_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'recplots'))\n",
    "DIRECT_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'direct'))\n",
    "GAF_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'gaf'))\n",
    "print(PROJ_ROOT)\n",
    "print(DATA_ROOT)\n",
    "print(PROCESSED_ROOT)\n",
    "import sys\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "CHANNEL_NAMES = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "                 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "META_COLUMN_NAMES = ['freq', 'RESP_4W', 'RESP_FIN', 'REMISE_FIN', 'AGE', 'SEX', 'M_1',\n",
    "       'M_4', 'M_F', 'délka léčby', 'lék 1', 'lék 2', 'lék 3', 'lék 4']\n",
    "META_FILE_NAME = 'DEP-POOL_Final_144.xlsx'\n",
    "meta_df = pd.read_excel(os.path.join(RAW_ROOT, META_FILE_NAME), index_col='ID', names=META_COLUMN_NAMES)\n",
    "\n",
    "raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '50a.fif'))\n",
    "t = pd.DataFrame(raw_fif.get_data())\n",
    "data = pd.DataFrame(np.transpose(t.values), columns=CHANNEL_NAMES).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welford's algorithm for computing running mean and variance\n",
    "def update(existingAggregate, newValues):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    for newValue in newValues: \n",
    "        count += 1\n",
    "        delta = newValue - mean\n",
    "        mean += delta / count\n",
    "        delta2 = newValue - mean\n",
    "        M2 += delta * delta2\n",
    "        existingAggregate = (count, mean, M2)\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "def finalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance, sampleVariance) = (mean, M2/count) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute recurrence plot / GAF (multichannel distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def rec_plot(data):\n",
    "    return squareform(pdist(data)).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, mean, M2 = 0, 0, 0\n",
    "\n",
    "def compute_vec(file, f, path, ww=256, maxl=np.infty):\n",
    "    global count, mean, M2\n",
    "    minl = ww\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    # for fn in os.listdir(path):\n",
    "    #     no_ext, _ = os.path.splitext(file.name)\n",
    "    #     if fn.startswith(no_ext):\n",
    "    #         print('Returning None, file ', no_ext, 'at teast partially processed')\n",
    "    #         return None\n",
    "    while start+ww <= min(maxl, len(file.df['FP1'].values)):\n",
    "        # Add third dimension to make Keras happy\n",
    "        r = np.zeros((ww, ww, 1))\n",
    "        data = file.df.values\n",
    "        # Here we may select only a subset of channels, let's try all for now\n",
    "        r[:,:,0] = f(data[start:start+ww, :])\n",
    "        if 2*len(data) < minl + ww or r.shape[0]*r.shape[1] != ww*ww:\n",
    "            print('Returning None, for file ', file.name, ', time series too short: ', len(data))\n",
    "            print('Or returned wrong shape: ', r.shape, start+ww)\n",
    "            return None\n",
    "        count, mean, M2 = update((count, mean, M2), r.reshape(-1))\n",
    "        np.save(\n",
    "            os.path.join(path, ''.join((str(file.id), file.trial, '-', str(chunk_num), '.npy'))), r, fix_imports=False)\n",
    "        start += ww\n",
    "        chunk_num += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0\n",
      "Processed:  1\n",
      "Processed:  2\n",
      "Processed:  3\n",
      "Processed:  4\n",
      "Processed:  5\n",
      "Processed:  6\n",
      "Processed:  7\n",
      "Processed:  8\n",
      "Processed:  9\n",
      "Processed:  10\n",
      "Processed:  11\n",
      "Processed:  12\n",
      "Processed:  13\n",
      "Processed:  14\n",
      "Processed:  15\n",
      "Processed:  16\n",
      "Processed:  17\n",
      "Processed:  18\n",
      "Processed:  19\n",
      "Processed:  20\n",
      "Processed:  21\n",
      "Processed:  22\n",
      "Processed:  23\n",
      "Processed:  24\n",
      "Processed:  25\n",
      "Processed:  26\n",
      "Processed:  27\n",
      "Processed:  28\n",
      "Processed:  29\n",
      "Processed:  30\n",
      "Processed:  31\n",
      "Processed:  32\n",
      "Processed:  33\n",
      "Processed:  34\n",
      "Processed:  35\n",
      "Processed:  36\n",
      "Processed:  37\n",
      "Processed:  38\n",
      "Processed:  39\n",
      "Processed:  40\n",
      "Processed:  41\n",
      "Processed:  42\n",
      "Processed:  43\n",
      "Processed:  44\n",
      "Processed:  45\n",
      "Processed:  46\n",
      "Processed:  47\n",
      "Processed:  48\n",
      "Processed:  49\n",
      "Processed:  50\n",
      "Processed:  51\n",
      "Processed:  52\n",
      "Processed:  53\n",
      "Processed:  54\n",
      "Processed:  55\n",
      "Processed:  56\n",
      "Processed:  57\n",
      "Processed:  58\n",
      "Processed:  59\n",
      "Processed:  60\n",
      "Processed:  61\n",
      "Processed:  62\n",
      "Processed:  63\n",
      "Processed:  64\n",
      "Processed:  65\n",
      "Processed:  66\n",
      "Processed:  67\n",
      "Processed:  68\n",
      "Processed:  69\n",
      "Processed:  70\n",
      "Processed:  71\n",
      "Processed:  72\n",
      "Processed:  73\n",
      "Processed:  74\n",
      "Processed:  75\n",
      "Processed:  76\n",
      "Processed:  77\n",
      "Processed:  78\n",
      "Processed:  79\n",
      "Processed:  80\n",
      "Processed:  81\n",
      "Processed:  82\n",
      "Processed:  83\n",
      "Processed:  84\n",
      "Processed:  85\n",
      "Processed:  86\n",
      "Processed:  87\n",
      "Processed:  88\n",
      "Processed:  89\n",
      "Processed:  90\n",
      "Processed:  91\n",
      "Processed:  92\n",
      "Processed:  93\n",
      "Processed:  94\n",
      "Processed:  95\n",
      "Processed:  96\n",
      "Processed:  97\n",
      "Processed:  98\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "count, mean, M2 = 0, 0, 0\n",
    "for i, file in enumerate(files_builder(DataKind('processed'))):\n",
    "    compute_vec(file, rec_plot, os.path.join(REC_ROOT, 'vectors'))\n",
    "    print('Processed: ', i)\n",
    "\n",
    "mean, st = finalize((count, mean, M2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(REC_ROOT, 'vectors')\n",
    "for fn in os.listdir(path):\n",
    "    filepath = os.path.join(path, fn)\n",
    "    r = np.load(filepath)\n",
    "    r = (r-mean) / std\n",
    "    # assert (r >= -1).all() and (r <= 1).all()\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute recurrence plot / GAF (separate channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def rec_plot(s):\n",
    "    return squareform(pdist(s[:,None])).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tabulate(x, y, f):\n",
    "    return np.vectorize(f)(*np.meshgrid(x, y, sparse=True))\n",
    "\n",
    "def cos_sum(x, y):\n",
    "    return math.cos(x+y)\n",
    "\n",
    "def gaf(serie):\n",
    "    # Min-Max scaling\n",
    "    min_ = np.amin(serie)\n",
    "    max_ = np.amax(serie)\n",
    "    scaled_serie = (2*serie - max_ - min_)/(max_ - min_)\n",
    "\n",
    "    # Floating point inaccuracy!\n",
    "    scaled_serie = np.where(scaled_serie >= 1., 1., scaled_serie)\n",
    "    scaled_serie = np.where(scaled_serie <= -1., -1., scaled_serie)\n",
    "\n",
    "    # Polar encoding\n",
    "    phi = np.arccos(scaled_serie)\n",
    "\n",
    "    # GAF Computation (every term of the matrix)\n",
    "    gaf = tabulate(phi, phi, cos_sum)\n",
    "\n",
    "    return gaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sep(file, f, path, ww=256, maxl=np.infty):\n",
    "    minl = ww\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    while start+ww <= min(maxl, len(file.df['FP1'].values)):\n",
    "        for i, channel in enumerate(CHANNEL_NAMES):\n",
    "            # file_found = False\n",
    "            # for fn in os.listdir(os.path.join(path, channel)):\n",
    "            #     no_ext, _ = os.path.splitext(file.name)\n",
    "            #     if fn.startswith(no_ext):\n",
    "            #         print('File ', fn, ' already processed, for channel ', channel, ', skipping...')\n",
    "            #         file_found = True\n",
    "            #         break\n",
    "            # if file_found: continue\n",
    "            data = file.df[channel].values\n",
    "            r = f(data[start:start+ww])\n",
    "            if 2*len(data) < minl + ww or r.shape[0]*r.shape[1] != ww*ww:\n",
    "                print('Skipping, file ', file.name, ', time series too short: ', len(data))\n",
    "                print('Or returned wrong shape: ', r.shape, start+ww)\n",
    "                continue\n",
    "            counts[i], means[i], M2s[i] = update((counts[i], means[i], M2s[i]), r.reshape(-1))\n",
    "            np.save(\n",
    "                os.path.join(path, channel, ''.join((str(file.id), file.trial, '-', str(chunk_num), '.npy'))),\n",
    "                r, fix_imports=False)\n",
    "        start += ww\n",
    "        chunk_num += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-a5501b32543a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_log_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataKind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrec_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREC_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sep_channels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# compute(file, gaf, GAF_ROOT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-133-6c58f950744f>\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(file, f, path, ww, maxl)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Or returned wrong shape: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mww\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             np.save(\n\u001b[1;32m     23\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-81e357d64a2c>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(existingAggregate, newValues)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnewValue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewValues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewValue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdelta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewValue\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "counts, means, M2s = np.zeros(19), np.zeros(19), np.zeros(19)\n",
    "final_means, final_stds = np.zeros(19), np.zeros(19)\n",
    "\n",
    "for channel in CHANNEL_NAMES:\n",
    "    if not os.path.exists(os.path.join(REC_ROOT, 'sep_channels', channel)):\n",
    "        os.makedirs(os.path.join(REC_ROOT, 'sep_channels', channel))\n",
    "\n",
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for file in files_builder(DataKind('processed')):\n",
    "    compute_sep(file, rec_plot, os.path.join(REC_ROOT, 'sep_channels'))\n",
    "    # compute_sep(file, gaf, GAF_ROOT)\n",
    "    print('Processed: ', i)\n",
    "    \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Saving...')\n",
    "for fn in os.listdir(path):\n",
    "    filepath = os.path.join(path,fn)\n",
    "    r = np.load(filepath)\n",
    "    for i in np.arange(r.shape[2]):\n",
    "        r[:, :, i] = (r[:, :, i]-final_means[i])/final_stds[i]\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing means and variances...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kovar/thesis_project/data/recplots_1/46a-48.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-2deb807074fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recplots_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/kovar/thesis_project/data/recplots_1/46a-48.npy'"
     ]
    }
   ],
   "source": [
    "print('Computing means and variances...')\n",
    "path = os.path.join(DATA_ROOT, 'recplots_1')\n",
    "for fn in os.listdir(path):\n",
    "    r = np.load(os.path.join(path,fn))\n",
    "    rs = r.reshape((-1, 19))\n",
    "    for i in np.arange(r.shape[2]):\n",
    "        counts[i], means[i], M2s[i] = update((counts[i], means[i], M2s[i]), rs[:, i])\n",
    "        \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))\n",
    "    \n",
    "print('Saving...')\n",
    "for fn in os.listdir(path):\n",
    "    filepath = os.path.join(path,fn)\n",
    "    r = np.load(filepath)\n",
    "    for i in np.arange(r.shape[2]):\n",
    "        r[:, :, i] = (r[:, :, i]-final_means[i])/final_stds[i]\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly to normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, means, M2s = np.zeros(19), np.zeros(19), np.zeros(19)\n",
    "final_means, final_stds = np.zeros(19), np.zeros(19)\n",
    "\n",
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for file in files_builder(DataKind('processed')):\n",
    "    compute_vec(file, direct, os.path.join(DIRECT_ROOT))\n",
    "    # compute(file, gaf, GAF_ROOT)\n",
    "    \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "# from measures import algorithms as algos\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_floatx('float32')\n",
    "K.floatx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "num_channels = 1\n",
    "batch_size = 64\n",
    "num_epochs= 35\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T6']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "\n",
    "ind = [False, False, False, False, False,\n",
    "       False, False, False, False, False, \n",
    "       False, False, False, False, False, \n",
    "       True, False, False, False]\n",
    "\n",
    "print(np.array(CHANNEL_NAMES)[ind])\n",
    "\n",
    "class batch_generator(Sequence):\n",
    "\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.n = len(self.filenames)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "        # return len(self.filenames) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:min(self.n, (idx + 1) * self.batch_size)]\n",
    "        batch_y = self.labels[idx * self.batch_size:min(self.n, (idx + 1) * self.batch_size)]\n",
    "        # assert len(batch_x) == batch_size, batch_x\n",
    "        # assert len(batch_y) == batch_size, batch_y\n",
    "\n",
    "        return np.array([\n",
    "            np.load(file_name)[:, :, :] \\\n",
    "            # np.expand_dims(np.load(file_name)[:, :, ind], axis=-1) \\\n",
    "            # np.expand_dims(np.mean(np.load(file_name)[:, :, ind], axis=-1), axis=-1) \\\n",
    "            for file_name in batch_x]), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_1(dropout_rate=dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(image_size, image_size, num_channels), data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(16, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(8, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(image_size,image_size,num_channels)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_3():\n",
    "    model = Sequential()\n",
    "    ki = initializers.RandomNormal(0, 0.1, 23)\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(image_size,image_size,num_channels),\n",
    "                     kernel_initializer=ki))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=ki))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(2, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4, beta_1=0.999, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_middle(filenames, labels):\n",
    "    ta = zip(filenames, labels)\n",
    "    ta = np.array([(t, l) for t, l in ta], dtype=[('fname', 'S100'), ('label', 'int8')])\n",
    "    ta = ta[:][(ta['label'] == -1) | (ta['label'] == 1)]\n",
    "    ta['label'][ta['label'] == -1] = 0\n",
    "    return ta['fname'].astype(str, copy=False), ta['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distribution:  {0: 6801, 1: 6006}\n",
      "Training distribution:  {0: 4718, 1: 4246}\n",
      "Testing distribution:  {0: 2083, 1: 1760}\n"
     ]
    }
   ],
   "source": [
    "fb = files_builder(DataKind('recplot'), subfolder=('vectors',))\n",
    "seed = 123\n",
    "fns = [fn[1] for fn in fb.file_names(include_path=True, subfolder=())]\n",
    "filenames, labels = remove_middle(fns, fb.get_labels(fns))\n",
    "# labels = to_categorical(labels)\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print('Overall distribution: ', dict(zip(unique, counts)))\n",
    "training_filenames, validation_filenames, training_labels, validation_labels = \\\n",
    "    train_test_split(filenames, labels, test_size=0.3, random_state=seed)\n",
    "assert len(training_filenames) == len(training_labels)\n",
    "assert len(validation_filenames) == len(validation_labels)\n",
    "unique, counts = np.unique(training_labels, return_counts=True)\n",
    "print('Training distribution: ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "print('Testing distribution: ', dict(zip(unique, counts)))\n",
    "training_labels = to_categorical(training_labels, dtype=training_labels.dtype)\n",
    "validation_labels = to_categorical(validation_labels, dtype=validation_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "141/141 [==============================] - 21s 146ms/step - loss: 0.6890 - acc: 0.5320 - val_loss: 0.6799 - val_acc: 0.5635\n",
      "Epoch 2/35\n",
      "141/141 [==============================] - 22s 153ms/step - loss: 0.6846 - acc: 0.5447 - val_loss: 0.6826 - val_acc: 0.5687\n",
      "Epoch 3/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.6801 - acc: 0.5578 - val_loss: 0.6748 - val_acc: 0.5717\n",
      "Epoch 4/35\n",
      "141/141 [==============================] - 22s 158ms/step - loss: 0.6735 - acc: 0.5696 - val_loss: 0.6723 - val_acc: 0.5701\n",
      "Epoch 5/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.6655 - acc: 0.5782 - val_loss: 0.6705 - val_acc: 0.5755\n",
      "Epoch 6/35\n",
      "141/141 [==============================] - 22s 157ms/step - loss: 0.6559 - acc: 0.6006 - val_loss: 0.6704 - val_acc: 0.5869\n",
      "Epoch 7/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.6426 - acc: 0.6204 - val_loss: 0.6745 - val_acc: 0.5787\n",
      "Epoch 8/35\n",
      "141/141 [==============================] - 22s 158ms/step - loss: 0.6305 - acc: 0.6320 - val_loss: 0.6723 - val_acc: 0.5880\n",
      "Epoch 9/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.6223 - acc: 0.6414 - val_loss: 0.6679 - val_acc: 0.5837\n",
      "Epoch 10/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.6084 - acc: 0.6592 - val_loss: 0.6701 - val_acc: 0.5890\n",
      "Epoch 11/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.6003 - acc: 0.6687 - val_loss: 0.6754 - val_acc: 0.5873\n",
      "Epoch 12/35\n",
      "141/141 [==============================] - 23s 162ms/step - loss: 0.5885 - acc: 0.6797 - val_loss: 0.7604 - val_acc: 0.5820\n",
      "Epoch 13/35\n",
      "141/141 [==============================] - 23s 161ms/step - loss: 0.5768 - acc: 0.6848 - val_loss: 0.7021 - val_acc: 0.5801\n",
      "Epoch 14/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5751 - acc: 0.6946 - val_loss: 0.6903 - val_acc: 0.5963\n",
      "Epoch 15/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.5609 - acc: 0.6993 - val_loss: 0.7134 - val_acc: 0.5759\n",
      "Epoch 16/35\n",
      "141/141 [==============================] - 23s 165ms/step - loss: 0.5549 - acc: 0.7072 - val_loss: 0.7168 - val_acc: 0.5902\n",
      "Epoch 17/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5407 - acc: 0.7192 - val_loss: 0.7161 - val_acc: 0.5891\n",
      "Epoch 18/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5276 - acc: 0.7229 - val_loss: 0.7172 - val_acc: 0.5734\n",
      "Epoch 19/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5315 - acc: 0.7241 - val_loss: 0.7200 - val_acc: 0.5814\n",
      "Epoch 20/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5201 - acc: 0.7334 - val_loss: 0.7260 - val_acc: 0.5813\n",
      "Epoch 21/35\n",
      "141/141 [==============================] - 23s 165ms/step - loss: 0.5174 - acc: 0.7364 - val_loss: 0.7154 - val_acc: 0.5817\n",
      "Epoch 22/35\n",
      "141/141 [==============================] - 23s 165ms/step - loss: 0.5155 - acc: 0.7381 - val_loss: 0.7226 - val_acc: 0.5721\n",
      "Epoch 23/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.5100 - acc: 0.7415 - val_loss: 0.7239 - val_acc: 0.5889\n",
      "Epoch 24/35\n",
      "141/141 [==============================] - 23s 162ms/step - loss: 0.5007 - acc: 0.7459 - val_loss: 0.7293 - val_acc: 0.5838\n",
      "Epoch 25/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4970 - acc: 0.7488 - val_loss: 0.7918 - val_acc: 0.5868\n",
      "Epoch 26/35\n",
      "141/141 [==============================] - 23s 162ms/step - loss: 0.4960 - acc: 0.7498 - val_loss: 0.7447 - val_acc: 0.5853\n",
      "Epoch 27/35\n",
      "141/141 [==============================] - 23s 165ms/step - loss: 0.4875 - acc: 0.7571 - val_loss: 0.7692 - val_acc: 0.5770\n",
      "Epoch 28/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.4900 - acc: 0.7544 - val_loss: 0.7475 - val_acc: 0.5889\n",
      "Epoch 29/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4794 - acc: 0.7600 - val_loss: 0.8548 - val_acc: 0.5808\n",
      "Epoch 30/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4755 - acc: 0.7640 - val_loss: 0.7760 - val_acc: 0.5852\n",
      "Epoch 31/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4701 - acc: 0.7680 - val_loss: 0.7592 - val_acc: 0.5934\n",
      "Epoch 32/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.4755 - acc: 0.7629 - val_loss: 0.7641 - val_acc: 0.5855\n",
      "Epoch 33/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4647 - acc: 0.7730 - val_loss: 0.7718 - val_acc: 0.5844\n",
      "Epoch 34/35\n",
      "141/141 [==============================] - 23s 163ms/step - loss: 0.4491 - acc: 0.7802 - val_loss: 0.8205 - val_acc: 0.5881\n",
      "Epoch 35/35\n",
      "141/141 [==============================] - 23s 164ms/step - loss: 0.4532 - acc: 0.7812 - val_loss: 0.8076 - val_acc: 0.5818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4d1c6f7da0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_batch_generator = batch_generator(training_filenames, training_labels, batch_size)\n",
    "validation_batch_generator = batch_generator(validation_filenames, validation_labels, batch_size)\n",
    "\n",
    "model = define_model_1()\n",
    "model.fit_generator(generator=training_batch_generator,\n",
    "                                      # steps_per_epoch=(len(training_filenames) // batch_size),\n",
    "                                      steps_per_epoch=len(training_batch_generator),\n",
    "                                      epochs=num_epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=validation_batch_generator,\n",
    "                                      # validation_steps=(len(validation_filenames) // batch_size),\n",
    "                                      validation_steps=len(validation_batch_generator),\n",
    "                                      use_multiprocessing=True,\n",
    "                                      workers=8,\n",
    "                                      max_queue_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.46"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    number_size = 4.0\n",
    "    if K.floatx() == 'float16':\n",
    "         number_size = 2.0\n",
    "    if K.floatx() == 'float64':\n",
    "         number_size = 8.0\n",
    "\n",
    "    total_memory = number_size*(batch_size*shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes\n",
    "get_model_memory_usage(128, define_model_2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_37 (Conv2D)           (None, 254, 254, 32)      320       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 254, 254, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 127, 127, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 125, 125, 16)      4624      \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 125, 125, 16)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 62, 62, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 60, 60, 8)         1160      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 60, 60, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 30, 30, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 7200)              0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 8)                 57608     \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 18        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 63,730\n",
      "Trainable params: 63,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = define_model_1()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "K.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
