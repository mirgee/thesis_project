{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 19 2019 \n",
      "\n",
      "CPython 3.6.6\n",
      "IPython 6.5.0\n",
      "\n",
      "numpy 1.15.1\n",
      "scipy 1.1.0\n",
      "sklearn 0.19.1\n",
      "pandas 0.23.4\n",
      "\n",
      "compiler   : GCC 4.8.2 20140120 (Red Hat 4.8.2-15)\n",
      "system     : Linux\n",
      "release    : 4.9.0-7-amd64\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 12\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kovar/thesis_project/\n",
      "/home/kovar/thesis_project/data\n",
      "/home/kovar/thesis_project/data/processed\n",
      "Opening raw data file /home/kovar/thesis_project/data/processed/50a.fif...\n",
      "This filename (/home/kovar/thesis_project/data/processed/50a.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "Isotrak not found\n",
      "    Range : 0 ... 16930 =      0.000 ...    67.720 secs\n",
      "Ready.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-5e950e664aee>:33: RuntimeWarning: This filename (/home/kovar/thesis_project/data/processed/50a.fif) does not conform to MNE naming conventions. All raw files should end with raw.fif, raw_sss.fif, raw_tsss.fif, raw.fif.gz, raw_sss.fif.gz or raw_tsss.fif.gz\n",
      "  raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '50a.fif'))\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nolds\n",
    "import data\n",
    "import mne\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "from measures import algorithms as algos\n",
    "\n",
    "PROJ_ROOT = os.getenv('THESIS_ROOT')\n",
    "DATA_ROOT = os.path.abspath(os.path.join(PROJ_ROOT, 'data'))\n",
    "PROCESSED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'processed'))\n",
    "RAW_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'raw'))\n",
    "LABELED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'labeled'))\n",
    "DURATIONS_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'durations'))\n",
    "REC_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'recplots'))\n",
    "print(PROJ_ROOT)\n",
    "print(DATA_ROOT)\n",
    "print(PROCESSED_ROOT)\n",
    "import sys\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "CHANNEL_NAMES = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "                 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "META_COLUMN_NAMES = ['freq', 'RESP_4W', 'RESP_FIN', 'REMISE_FIN', 'AGE', 'SEX', 'M_1',\n",
    "       'M_4', 'M_F', 'délka léčby', 'lék 1', 'lék 2', 'lék 3', 'lék 4']\n",
    "META_FILE_NAME = 'DEP-POOL_Final_144.xlsx'\n",
    "meta_df = pd.read_excel(os.path.join(RAW_ROOT, META_FILE_NAME), index_col='ID', names=META_COLUMN_NAMES)\n",
    "\n",
    "raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '50a.fif'))\n",
    "t = pd.DataFrame(raw_fif.get_data())\n",
    "data = pd.DataFrame(np.transpose(t.values), columns=CHANNEL_NAMES)\n",
    "data = np.transpose(data.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute recurrence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def rec_plot(s, eps=0.10, steps=10):\n",
    "    d = pdist(s[:,None])\n",
    "    d = np.floor(d/eps)\n",
    "    d[d>steps] = steps\n",
    "    Z = squareform(d).astype('int8')\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rp(file, minl=1000, maxl=1000):\n",
    "    res = np.zeros((maxl, maxl, len(CHANNEL_NAMES)))\n",
    "    for i, channel in enumerate(CHANNEL_NAMES):\n",
    "        data = file.df[channel].values\n",
    "        length = len(data) if maxl is None else min(maxl, len(data))\n",
    "        res[:, :, i] = rec_plot(data[:length])\n",
    "    np.save(os.path.join(REC_ROOT, ''.join((str(file.id), file.trial, '.npy'))), res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for file in files_builder(DataKind('processed')):\n",
    "    res = compute_rp(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 1000\n",
    "num_channels = 1\n",
    "batch_size = 10\n",
    "num_epochs=25\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "\n",
    "class batch_generator(Sequence):\n",
    "\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        # TODO: Here we can select the channels\n",
    "        return np.array([\n",
    "            np.expand_dims(np.load(file_name.decode('ascii')), axis=-1) for file_name in batch_x]), np.array(batch_y).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(dropout_rate=dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(image_size, image_size, num_channels), batch_size=batch_size, data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_middle(filenames, labels):\n",
    "    ta = zip(filenames, labels)\n",
    "    ta = np.array([(t, l) for t, l in ta], dtype=[('fname', 'S100'), ('label', 'int8')])\n",
    "    ta = ta[:][(ta['label'] == -1) | (ta['label'] == 1)]\n",
    "    return ta['fname'], ta['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distribution:  {-1: 62, 1: 56}\n",
      "Training distribution:  {-1: 45, 1: 37}\n",
      "Testing distribution:  {-1: 17, 1: 19}\n"
     ]
    }
   ],
   "source": [
    "fb = files_builder(DataKind('recplot'))\n",
    "seed = 123\n",
    "fns = [fn[1] for fn in fb.file_names(True)]\n",
    "filenames, labels = remove_middle(fns, fb.get_labels())\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print('Overall distribution: ', dict(zip(unique, counts)))\n",
    "training_filenames, validation_filenames, training_labels, validation_labels = \\\n",
    "    train_test_split(filenames, labels, test_size=0.3, random_state=seed)\n",
    "unique, counts = np.unique(training_labels, return_counts=True)\n",
    "print('Training distribution: ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "print('Testing distribution: ', dict(zip(unique, counts)))\n",
    "training_labels = to_categorical(training_labels, 2)\n",
    "validation_labels = to_categorical(validation_labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    }
   ],
   "source": [
    "training_batch_generator = batch_generator(training_filenames, training_labels, batch_size)\n",
    "validation_batch_generator = batch_generator(validation_filenames, validation_labels, batch_size)\n",
    "\n",
    "model = define_model()\n",
    "model.fit_generator(generator=training_batch_generator,\n",
    "                                      steps_per_epoch=(len(training_filenames) // batch_size),\n",
    "                                      epochs=num_epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=validation_batch_generator,\n",
    "                                      validation_steps=(len(validation_filenames) // batch_size),\n",
    "                                      use_multiprocessing=False,)\n",
    "                                      # workers=None,\n",
    "                                      # max_queue_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = define_model()\n",
    "model.fit(\n",
    "        train_dataset, train_labels,\n",
    "        batch_size=batch_size,\n",
    "#         steps_per_epoch=2000 // batch_size,\n",
    "        epochs=epochs,\n",
    "#         validation_data=(valid_dataset, valid_labels),\n",
    "#         validation_steps=800 // batch_size\n",
    "        )\n",
    "\n",
    "print(model.evaluate(test_dataset, test_labels, batch_size=batch_size))\n",
    "# model.save_weights('.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
