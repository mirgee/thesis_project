{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The watermark extension is already loaded. To reload it, use:\n",
      "  %reload_ext watermark\n",
      "Thu Jan 24 2019 \n",
      "\n",
      "CPython 3.6.6\n",
      "IPython 6.5.0\n",
      "\n",
      "numpy 1.15.1\n",
      "scipy 1.1.0\n",
      "sklearn 0.19.1\n",
      "pandas 0.23.4\n",
      "\n",
      "compiler   : GCC 4.8.2 20140120 (Red Hat 4.8.2-15)\n",
      "system     : Linux\n",
      "release    : 4.20.3-arch1-1-ARCH\n",
      "machine    : x86_64\n",
      "processor  : \n",
      "CPU cores  : 16\n",
      "interpreter: 64bit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext watermark\n",
    "%watermark -v -n -m -p numpy,scipy,sklearn,pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kovar/thesis_project/\n",
      "/home/kovar/thesis_project/data\n",
      "/home/kovar/thesis_project/data/processed\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nolds\n",
    "import data\n",
    "import mne\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "\n",
    "PROJ_ROOT = os.getenv('THESIS_ROOT')\n",
    "DATA_ROOT = os.path.abspath(os.path.join(PROJ_ROOT, 'data'))\n",
    "PROCESSED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'processed'))\n",
    "RAW_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'raw'))\n",
    "LABELED_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'labeled'))\n",
    "DURATIONS_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'durations'))\n",
    "REC_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'recplots'))\n",
    "DIRECT_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'direct'))\n",
    "GAF_ROOT = os.path.abspath(os.path.join(DATA_ROOT, 'gaf'))\n",
    "print(PROJ_ROOT)\n",
    "print(DATA_ROOT)\n",
    "print(PROCESSED_ROOT)\n",
    "import sys\n",
    "sys.path.append(os.path.join(PROJ_ROOT, 'src'))\n",
    "CHANNEL_NAMES = ['FP1', 'FP2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2',\n",
    "                 'F7', 'F8', 'T3', 'T4', 'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "META_COLUMN_NAMES = ['freq', 'RESP_4W', 'RESP_FIN', 'REMISE_FIN', 'AGE', 'SEX', 'M_1',\n",
    "       'M_4', 'M_F', 'délka léčby', 'lék 1', 'lék 2', 'lék 3', 'lék 4']\n",
    "META_FILE_NAME = 'DEP-POOL_Final_144.xlsx'\n",
    "meta_df = pd.read_excel(os.path.join(RAW_ROOT, META_FILE_NAME), index_col='ID', names=META_COLUMN_NAMES)\n",
    "\n",
    "raw_fif = mne.io.read_raw_fif(os.path.join(PROCESSED_ROOT, '50a.fif'))\n",
    "t = pd.DataFrame(raw_fif.get_data())\n",
    "data = pd.DataFrame(np.transpose(t.values), columns=CHANNEL_NAMES).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Welford's algorithm for computing running mean and variance\n",
    "def update(existingAggregate, newValues):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    for newValue in newValues: \n",
    "        count += 1\n",
    "        delta = newValue - mean\n",
    "        mean += delta / count\n",
    "        delta2 = newValue - mean\n",
    "        M2 += delta * delta2\n",
    "        existingAggregate = (count, mean, M2)\n",
    "\n",
    "    return (count, mean, M2)\n",
    "\n",
    "def finalize(existingAggregate):\n",
    "    (count, mean, M2) = existingAggregate\n",
    "    (mean, variance) = (mean, M2/count) \n",
    "    if count < 2:\n",
    "        return float('nan')\n",
    "    else:\n",
    "        return (mean, np.sqrt(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algos\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import math\n",
    "\n",
    "def rec_plot(data):\n",
    "    if len(data.shape) > 1:\n",
    "        return squareform(pdist(data)).astype('float64')\n",
    "    else:\n",
    "        return squareform(pdist(data[:,None])).astype('float64')\n",
    "\n",
    "def tabulate(x, y, f):\n",
    "    return np.vectorize(f)(*np.meshgrid(x, y, sparse=True))\n",
    "\n",
    "def cos_sum(x, y):\n",
    "    return math.cos(x+y)\n",
    "\n",
    "def gaf(serie):\n",
    "    # Min-Max scaling\n",
    "    min_ = np.amin(serie)\n",
    "    max_ = np.amax(serie)\n",
    "    scaled_serie = (2*serie - max_ - min_)/(max_ - min_)\n",
    "\n",
    "    # Floating point inaccuracy!\n",
    "    scaled_serie = np.where(scaled_serie >= 1., 1., scaled_serie)\n",
    "    scaled_serie = np.where(scaled_serie <= -1., -1., scaled_serie)\n",
    "\n",
    "    # Polar encoding\n",
    "    phi = np.arccos(scaled_serie)\n",
    "\n",
    "    # GAF Computation (every term of the matrix)\n",
    "    gaf = tabulate(phi, phi, cos_sum)\n",
    "\n",
    "    return gaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute recurrence plot / GAF (multichannel distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "count, mean, M2 = 0, 0, 0\n",
    "\n",
    "def compute_vec(file, f, path, ww=256, maxl=np.infty):\n",
    "    global count, mean, M2\n",
    "    minl = ww\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    # for fn in os.listdir(path):\n",
    "    #     no_ext, _ = os.path.splitext(file.name)\n",
    "    #     if fn.startswith(no_ext):\n",
    "    #         print('Returning None, file ', no_ext, 'at teast partially processed')\n",
    "    #         return None\n",
    "    while start+ww <= min(maxl, len(file.df['FP1'].values)):\n",
    "        # Add third dimension to make Keras happy\n",
    "        r = np.zeros((ww, ww, 1))\n",
    "        data = file.df.values\n",
    "        # Here we may select only a subset of channels, let's try all for now\n",
    "        r[:,:,0] = f(data[start:start+ww, :])\n",
    "        if 2*len(data) < minl + ww or r.shape[0]*r.shape[1] != ww*ww:\n",
    "            print('Returning None, for file ', file.name, ', time series too short: ', len(data))\n",
    "            print('Or returned wrong shape: ', r.shape, start+ww)\n",
    "            return None\n",
    "        count, mean, M2 = update((count, mean, M2), r.reshape(-1))\n",
    "        np.save(\n",
    "            os.path.join(path, ''.join((str(file.id), file.trial, '-', str(chunk_num), '.npy'))), r, fix_imports=False)\n",
    "        start += ww\n",
    "        chunk_num += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0\n",
      "Processed:  1\n",
      "Processed:  2\n",
      "Processed:  3\n",
      "Processed:  4\n",
      "Processed:  5\n",
      "Processed:  6\n",
      "Processed:  7\n",
      "Processed:  8\n",
      "Processed:  9\n",
      "Processed:  10\n",
      "Processed:  11\n",
      "Processed:  12\n",
      "Processed:  13\n",
      "Processed:  14\n",
      "Processed:  15\n",
      "Processed:  16\n",
      "Processed:  17\n",
      "Processed:  18\n",
      "Processed:  19\n",
      "Processed:  20\n",
      "Processed:  21\n",
      "Processed:  22\n",
      "Processed:  23\n",
      "Processed:  24\n",
      "Processed:  25\n",
      "Processed:  26\n",
      "Processed:  27\n",
      "Processed:  28\n",
      "Processed:  29\n",
      "Processed:  30\n",
      "Processed:  31\n",
      "Processed:  32\n",
      "Processed:  33\n",
      "Processed:  34\n",
      "Processed:  35\n",
      "Processed:  36\n",
      "Processed:  37\n",
      "Processed:  38\n",
      "Processed:  39\n",
      "Processed:  40\n",
      "Processed:  41\n",
      "Processed:  42\n",
      "Processed:  43\n",
      "Processed:  44\n",
      "Processed:  45\n",
      "Processed:  46\n",
      "Processed:  47\n",
      "Processed:  48\n",
      "Processed:  49\n",
      "Processed:  50\n",
      "Processed:  51\n",
      "Processed:  52\n",
      "Processed:  53\n",
      "Processed:  54\n",
      "Processed:  55\n",
      "Processed:  56\n",
      "Processed:  57\n",
      "Processed:  58\n",
      "Processed:  59\n",
      "Processed:  60\n",
      "Processed:  61\n",
      "Processed:  62\n",
      "Processed:  63\n",
      "Processed:  64\n",
      "Processed:  65\n",
      "Processed:  66\n",
      "Processed:  67\n",
      "Processed:  68\n",
      "Processed:  69\n",
      "Processed:  70\n",
      "Processed:  71\n",
      "Processed:  72\n",
      "Processed:  73\n",
      "Processed:  74\n",
      "Processed:  75\n",
      "Processed:  76\n",
      "Processed:  77\n",
      "Processed:  78\n",
      "Processed:  79\n",
      "Processed:  80\n",
      "Processed:  81\n",
      "Processed:  82\n",
      "Processed:  83\n",
      "Processed:  84\n",
      "Processed:  85\n",
      "Processed:  86\n",
      "Processed:  87\n",
      "Processed:  88\n",
      "Processed:  89\n",
      "Processed:  90\n",
      "Processed:  91\n",
      "Processed:  92\n",
      "Processed:  93\n",
      "Processed:  94\n",
      "Processed:  95\n",
      "Processed:  96\n",
      "Processed:  97\n",
      "Processed:  98\n",
      "Processed:  99\n",
      "Processed:  100\n",
      "Processed:  101\n",
      "Processed:  102\n",
      "Processed:  103\n",
      "Processed:  104\n",
      "Processed:  105\n",
      "Processed:  106\n",
      "Processed:  107\n",
      "Processed:  108\n",
      "Processed:  109\n",
      "Processed:  110\n",
      "Processed:  111\n",
      "Processed:  112\n",
      "Processed:  113\n",
      "Processed:  114\n",
      "Processed:  115\n",
      "Processed:  116\n",
      "Processed:  117\n",
      "Processed:  118\n",
      "Processed:  119\n",
      "Processed:  120\n",
      "Processed:  121\n",
      "Processed:  122\n",
      "Processed:  123\n",
      "Processed:  124\n",
      "Processed:  125\n",
      "Processed:  126\n",
      "Processed:  127\n",
      "Processed:  128\n",
      "Processed:  129\n",
      "Processed:  130\n",
      "Processed:  131\n",
      "Processed:  132\n",
      "Processed:  133\n",
      "Processed:  134\n",
      "Processed:  135\n",
      "Processed:  136\n",
      "Processed:  137\n",
      "Processed:  138\n",
      "Processed:  139\n",
      "Processed:  140\n",
      "Processed:  141\n",
      "Processed:  142\n",
      "Processed:  143\n",
      "Processed:  144\n",
      "Processed:  145\n",
      "Processed:  146\n",
      "Processed:  147\n",
      "Processed:  148\n",
      "Processed:  149\n",
      "Processed:  150\n",
      "Processed:  151\n",
      "Processed:  152\n",
      "Processed:  153\n",
      "Processed:  154\n",
      "Processed:  155\n",
      "Processed:  156\n",
      "Processed:  157\n",
      "Processed:  158\n",
      "Processed:  159\n",
      "Processed:  160\n",
      "Processed:  161\n",
      "Processed:  162\n",
      "Processed:  163\n",
      "Processed:  164\n",
      "Processed:  165\n",
      "Processed:  166\n",
      "Processed:  167\n",
      "Processed:  168\n",
      "Processed:  169\n",
      "Processed:  170\n",
      "Processed:  171\n",
      "Processed:  172\n",
      "Processed:  173\n",
      "Processed:  174\n",
      "Processed:  175\n",
      "Processed:  176\n",
      "Processed:  177\n",
      "Processed:  178\n",
      "Processed:  179\n",
      "Processed:  180\n",
      "Processed:  181\n",
      "Processed:  182\n",
      "Processed:  183\n",
      "Processed:  184\n",
      "Processed:  185\n",
      "Processed:  186\n",
      "Processed:  187\n",
      "Processed:  188\n",
      "Processed:  189\n",
      "Processed:  190\n",
      "Processed:  191\n",
      "Processed:  192\n",
      "Processed:  193\n",
      "Processed:  194\n",
      "Processed:  195\n",
      "Processed:  196\n",
      "Processed:  197\n",
      "Processed:  198\n",
      "Processed:  199\n",
      "Processed:  200\n",
      "Processed:  201\n",
      "Processed:  202\n",
      "Processed:  203\n",
      "Processed:  204\n",
      "Processed:  205\n",
      "Processed:  206\n",
      "Processed:  207\n",
      "Processed:  208\n",
      "Processed:  209\n",
      "Processed:  210\n",
      "Processed:  211\n",
      "Processed:  212\n",
      "Processed:  213\n",
      "Processed:  214\n",
      "Processed:  215\n",
      "Processed:  216\n",
      "Processed:  217\n",
      "Processed:  218\n",
      "Processed:  219\n",
      "Processed:  220\n",
      "Processed:  221\n",
      "Processed:  222\n",
      "Processed:  223\n",
      "Processed:  224\n",
      "Processed:  225\n",
      "Processed:  226\n",
      "Processed:  227\n",
      "Processed:  228\n",
      "Processed:  229\n",
      "Processed:  230\n",
      "Processed:  231\n",
      "Processed:  232\n",
      "Processed:  233\n",
      "Processed:  234\n",
      "Processed:  235\n",
      "Processed:  236\n",
      "Processed:  237\n",
      "Processed:  238\n",
      "Processed:  239\n",
      "Processed:  240\n",
      "Processed:  241\n",
      "Processed:  242\n",
      "Processed:  243\n",
      "Processed:  244\n",
      "Processed:  245\n",
      "Processed:  246\n",
      "Processed:  247\n",
      "Processed:  248\n",
      "Processed:  249\n",
      "Processed:  250\n",
      "Processed:  251\n",
      "Processed:  252\n",
      "Processed:  253\n",
      "Processed:  254\n",
      "Processed:  255\n",
      "Processed:  256\n",
      "Processed:  257\n",
      "Processed:  258\n",
      "Processed:  259\n",
      "Processed:  260\n",
      "Processed:  261\n",
      "Processed:  262\n",
      "Processed:  263\n",
      "Processed:  264\n",
      "Processed:  265\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-f90e5cde9c31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processed: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-136-81e357d64a2c>\u001b[0m in \u001b[0;36mfinalize\u001b[0;34m(existingAggregate)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexistingAggregate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexistingAggregate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampleVariance\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "count, mean, M2 = 0, 0, 0\n",
    "for i, file in enumerate(files_builder(DataKind('processed'))):\n",
    "    compute_vec(file, rec_plot, os.path.join(REC_ROOT, 'vectors'))\n",
    "    print('Processed: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, st = finalize((count, mean, M2))\n",
    "path = os.path.join(REC_ROOT, 'vectors')\n",
    "for fn in os.listdir(path):\n",
    "    filepath = os.path.join(path, fn)\n",
    "    r = np.load(filepath)\n",
    "    r = (r-mean) / std\n",
    "    # assert (r >= -1).all() and (r <= 1).all()\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute recurrence plot / GAF (separate channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sep(file, f, path, ww=256, maxl=np.infty):\n",
    "    minl = ww\n",
    "    start = 0\n",
    "    chunk_num = 0\n",
    "    while start+ww <= min(maxl, len(file.df['FP1'].values)):\n",
    "        for i, channel in enumerate(CHANNEL_NAMES):\n",
    "            # file_found = False\n",
    "            # for fn in os.listdir(os.path.join(path, channel)):\n",
    "            #     no_ext, _ = os.path.splitext(file.name)\n",
    "            #     if fn.startswith(no_ext):\n",
    "            #         print('File ', fn, ' already processed, for channel ', channel, ', skipping...')\n",
    "            #         file_found = True\n",
    "            #         break\n",
    "            # if file_found: continue\n",
    "            data = file.df[channel].values\n",
    "            r = f(data[start:start+ww])\n",
    "            if 2*len(data) < minl + ww or r.shape[0]*r.shape[1] != ww*ww:\n",
    "                print('Skipping, file ', file.name, ', time series too short: ', len(data))\n",
    "                print('Or returned wrong shape: ', r.shape, start+ww)\n",
    "                continue\n",
    "            counts[i], means[i], M2s[i] = update((counts[i], means[i], M2s[i]), r.reshape(-1))\n",
    "            np.save(\n",
    "                os.path.join(path, channel, ''.join((str(file.id), file.trial, '-', str(chunk_num), '.npy'))),\n",
    "                r, fix_imports=False)\n",
    "        start += ww\n",
    "        chunk_num += 1\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed:  0\n",
      "Processed:  1\n",
      "Processed:  2\n",
      "Processed:  3\n",
      "Processed:  4\n",
      "Processed:  5\n",
      "Processed:  6\n",
      "Processed:  7\n",
      "Processed:  8\n",
      "Processed:  9\n",
      "Processed:  10\n",
      "Processed:  11\n",
      "Processed:  12\n",
      "Processed:  13\n",
      "Processed:  14\n",
      "Processed:  15\n",
      "Processed:  16\n",
      "Processed:  17\n",
      "Processed:  18\n",
      "Processed:  19\n",
      "Processed:  20\n",
      "Processed:  21\n",
      "Processed:  22\n",
      "Processed:  23\n",
      "Processed:  24\n",
      "Processed:  25\n",
      "Processed:  26\n",
      "Processed:  27\n",
      "Processed:  28\n",
      "Processed:  29\n",
      "Processed:  30\n",
      "Processed:  31\n",
      "Processed:  32\n",
      "Processed:  33\n",
      "Processed:  34\n",
      "Processed:  35\n",
      "Processed:  36\n",
      "Processed:  37\n",
      "Processed:  38\n",
      "Processed:  39\n",
      "Processed:  40\n",
      "Processed:  41\n",
      "Processed:  42\n",
      "Processed:  43\n",
      "Processed:  44\n",
      "Processed:  45\n",
      "Processed:  46\n",
      "Processed:  47\n",
      "Processed:  48\n",
      "Processed:  49\n",
      "Processed:  50\n",
      "Processed:  51\n",
      "Processed:  52\n",
      "Processed:  53\n",
      "Processed:  54\n",
      "Processed:  55\n",
      "Processed:  56\n",
      "Processed:  57\n",
      "Processed:  58\n",
      "Processed:  59\n",
      "Processed:  60\n",
      "Processed:  61\n",
      "Processed:  62\n",
      "Processed:  63\n",
      "Processed:  64\n",
      "Processed:  65\n",
      "Processed:  66\n",
      "Processed:  67\n",
      "Processed:  68\n",
      "Processed:  69\n",
      "Processed:  70\n",
      "Processed:  71\n",
      "Processed:  72\n",
      "Processed:  73\n",
      "Processed:  74\n",
      "Processed:  75\n",
      "Processed:  76\n",
      "Processed:  77\n",
      "Processed:  78\n",
      "Processed:  79\n",
      "Processed:  80\n",
      "Processed:  81\n",
      "Processed:  82\n",
      "Processed:  83\n",
      "Processed:  84\n",
      "Processed:  85\n",
      "Processed:  86\n",
      "Processed:  87\n",
      "Processed:  88\n",
      "Processed:  89\n",
      "Processed:  90\n",
      "Processed:  91\n",
      "Processed:  92\n",
      "Processed:  93\n",
      "Processed:  94\n",
      "Processed:  95\n",
      "Processed:  96\n",
      "Processed:  97\n",
      "Processed:  98\n",
      "Processed:  99\n",
      "Processed:  100\n",
      "Processed:  101\n",
      "Processed:  102\n",
      "Processed:  103\n",
      "Processed:  104\n",
      "Processed:  105\n",
      "Processed:  106\n",
      "Processed:  107\n",
      "Processed:  108\n",
      "Processed:  109\n",
      "Processed:  110\n",
      "Processed:  111\n",
      "Processed:  112\n",
      "Processed:  113\n",
      "Processed:  114\n",
      "Processed:  115\n",
      "Processed:  116\n",
      "Processed:  117\n",
      "Processed:  118\n",
      "Processed:  119\n",
      "Processed:  120\n",
      "Processed:  121\n",
      "Processed:  122\n",
      "Processed:  123\n",
      "Processed:  124\n",
      "Processed:  125\n",
      "Processed:  126\n",
      "Processed:  127\n",
      "Processed:  128\n",
      "Processed:  129\n",
      "Processed:  130\n",
      "Processed:  131\n",
      "Processed:  132\n",
      "Processed:  133\n",
      "Processed:  134\n",
      "Processed:  135\n",
      "Processed:  136\n",
      "Processed:  137\n",
      "Processed:  138\n",
      "Processed:  139\n",
      "Processed:  140\n",
      "Processed:  141\n",
      "Processed:  142\n",
      "Processed:  143\n",
      "Processed:  144\n",
      "Processed:  145\n",
      "Processed:  146\n",
      "Processed:  147\n",
      "Processed:  148\n",
      "Processed:  149\n",
      "Processed:  150\n",
      "Processed:  151\n",
      "Processed:  152\n",
      "Processed:  153\n",
      "Processed:  154\n",
      "Processed:  155\n",
      "Processed:  156\n",
      "Processed:  157\n",
      "Processed:  158\n",
      "Processed:  159\n",
      "Processed:  160\n",
      "Processed:  161\n",
      "Processed:  162\n",
      "Processed:  163\n",
      "Processed:  164\n",
      "Processed:  165\n",
      "Processed:  166\n",
      "Processed:  167\n",
      "Processed:  168\n",
      "Processed:  169\n",
      "Processed:  170\n",
      "Processed:  171\n",
      "Processed:  172\n",
      "Processed:  173\n",
      "Processed:  174\n",
      "Processed:  175\n",
      "Processed:  176\n",
      "Processed:  177\n",
      "Processed:  178\n",
      "Processed:  179\n",
      "Processed:  180\n",
      "Processed:  181\n",
      "Processed:  182\n",
      "Processed:  183\n",
      "Processed:  184\n",
      "Processed:  185\n",
      "Processed:  186\n",
      "Processed:  187\n",
      "Processed:  188\n",
      "Processed:  189\n",
      "Processed:  190\n",
      "Processed:  191\n",
      "Processed:  192\n",
      "Processed:  193\n",
      "Processed:  194\n",
      "Processed:  195\n",
      "Processed:  196\n",
      "Processed:  197\n",
      "Processed:  198\n",
      "Processed:  199\n",
      "Processed:  200\n",
      "Processed:  201\n",
      "Processed:  202\n",
      "Processed:  203\n",
      "Processed:  204\n",
      "Processed:  205\n",
      "Processed:  206\n",
      "Processed:  207\n",
      "Processed:  208\n",
      "Processed:  209\n",
      "Processed:  210\n",
      "Processed:  211\n",
      "Processed:  212\n",
      "Processed:  213\n",
      "Processed:  214\n",
      "Processed:  215\n",
      "Processed:  216\n",
      "Processed:  217\n",
      "Processed:  218\n",
      "Processed:  219\n",
      "Processed:  220\n",
      "Processed:  221\n",
      "Processed:  222\n",
      "Processed:  223\n",
      "Processed:  224\n",
      "Processed:  225\n",
      "Processed:  226\n",
      "Processed:  227\n",
      "Processed:  228\n",
      "Processed:  229\n",
      "Processed:  230\n",
      "Processed:  231\n",
      "Processed:  232\n",
      "Processed:  233\n",
      "Processed:  234\n",
      "Processed:  235\n",
      "Processed:  236\n",
      "Processed:  237\n",
      "Processed:  238\n",
      "Processed:  239\n",
      "Processed:  240\n",
      "Processed:  241\n",
      "Processed:  242\n",
      "Processed:  243\n",
      "Processed:  244\n",
      "Processed:  245\n",
      "Processed:  246\n",
      "Processed:  247\n",
      "Processed:  248\n",
      "Processed:  249\n",
      "Processed:  250\n",
      "Processed:  251\n",
      "Processed:  252\n",
      "Processed:  253\n",
      "Processed:  254\n",
      "Processed:  255\n",
      "Processed:  256\n",
      "Processed:  257\n",
      "Processed:  258\n",
      "Processed:  259\n",
      "Processed:  260\n",
      "Processed:  261\n",
      "Processed:  262\n",
      "Processed:  263\n",
      "Processed:  264\n",
      "Processed:  265\n",
      "Finalizing...\n"
     ]
    }
   ],
   "source": [
    "counts, means, M2s = np.zeros(19), np.zeros(19), np.zeros(19)\n",
    "final_means, final_stds = np.zeros(19), np.zeros(19)\n",
    "\n",
    "for channel in CHANNEL_NAMES:\n",
    "    if not os.path.exists(os.path.join(REC_ROOT, 'sep_channels', channel)):\n",
    "        os.makedirs(os.path.join(REC_ROOT, 'sep_channels', channel))\n",
    "\n",
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for i, file in enumerate(files_builder(DataKind('processed'))):\n",
    "    compute_sep(file, rec_plot, os.path.join(REC_ROOT, 'sep_channels'))\n",
    "    # compute_sep(file, gaf, GAF_ROOT)\n",
    "    print('Processed: ', i)\n",
    "    \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    }
   ],
   "source": [
    "print('Saving...')\n",
    "for i, channel in enumerate(CHANNEL_NAMES):\n",
    "    path = os.path.join(REC_ROOT, 'sep_channels', channel)\n",
    "    for fn in os.listdir(path):\n",
    "        filepath = os.path.join(path,fn)\n",
    "        r = np.load(filepath)\n",
    "        r[:, :] = (r[:, :]-final_means[i])/final_stds[i]\n",
    "        np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing means and variances...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/kovar/thesis_project/data/recplots_1/46a-48.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-2deb807074fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_ROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'recplots_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/thesis/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/kovar/thesis_project/data/recplots_1/46a-48.npy'"
     ]
    }
   ],
   "source": [
    "print('Computing means and variances...')\n",
    "path = os.path.join(DATA_ROOT, 'recplots_1')\n",
    "for fn in os.listdir(path):\n",
    "    r = np.load(os.path.join(path,fn))\n",
    "    rs = r.reshape((-1, 19))\n",
    "    for i in np.arange(r.shape[2]):\n",
    "        counts[i], means[i], M2s[i] = update((counts[i], means[i], M2s[i]), rs[:, i])\n",
    "        \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))\n",
    "    \n",
    "print('Saving...')\n",
    "for fn in os.listdir(path):\n",
    "    filepath = os.path.join(path,fn)\n",
    "    r = np.load(filepath)\n",
    "    for i in np.arange(r.shape[2]):\n",
    "        r[:, :, i] = (r[:, :, i]-final_means[i])/final_stds[i]\n",
    "    np.save(filepath, r, fix_imports=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly to normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct(data):\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, means, M2s = np.zeros(19), np.zeros(19), np.zeros(19)\n",
    "final_means, final_stds = np.zeros(19), np.zeros(19)\n",
    "\n",
    "import logging\n",
    "mne.set_log_level(logging.ERROR)\n",
    "for file in files_builder(DataKind('processed')):\n",
    "    compute_vec(file, direct, os.path.join(DIRECT_ROOT))\n",
    "    # compute(file, gaf, GAF_ROOT)\n",
    "    \n",
    "print('Finalizing...')\n",
    "for i in np.arange(len(CHANNEL_NAMES)):\n",
    "    (final_means[i], final_stds[i]) = finalize((counts[i], means[i], M2s[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "from data.data_files import CHANNEL_NAMES, DataKind, files_builder\n",
    "# from measures import algorithms as algos\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_floatx('float32')\n",
    "K.floatx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "num_channels = 1\n",
    "batch_size = 128\n",
    "num_epochs= 35\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T6']\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import Sequence\n",
    "\n",
    "ind = [False, False, False, False, False,\n",
    "       False, False, False, False, False, \n",
    "       False, False, False, False, False, \n",
    "       True, False, False, False]\n",
    "\n",
    "print(np.array(CHANNEL_NAMES)[ind])\n",
    "\n",
    "class batch_generator(Sequence):\n",
    "\n",
    "    def __init__(self, filenames, labels, batch_size):\n",
    "        self.filenames, self.labels = filenames, labels\n",
    "        self.n = len(self.filenames)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / float(self.batch_size)))\n",
    "        # return len(self.filenames) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.filenames[idx * self.batch_size:min(self.n, (idx + 1) * self.batch_size)]\n",
    "        batch_y = self.labels[idx * self.batch_size:min(self.n, (idx + 1) * self.batch_size)]\n",
    "        # assert len(batch_x) == batch_size, batch_x\n",
    "        # assert len(batch_y) == batch_size, batch_y\n",
    "\n",
    "        return np.array([\n",
    "            # np.load(file_name) \\\n",
    "            np.expand_dims(np.load(file_name), axis=-1) \\\n",
    "            # np.expand_dims(np.mean(np.load(file_name)[:, :, ind], axis=-1), axis=-1) \\\n",
    "            for file_name in batch_x]), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_1(dropout_rate=dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(image_size, image_size, num_channels), data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(16, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(8, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(8))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-like model\n",
    "def define_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(image_size,image_size,num_channels)))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nikita's model\n",
    "def define_model_3():\n",
    "    model = Sequential()\n",
    "    ki = initializers.RandomNormal(0, 0.1, 23)\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(image_size,image_size,num_channels),\n",
    "                     kernel_initializer=ki))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.75))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer=ki))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(2, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-4, beta_1=0.999, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 'Classification of Recurrence Plots’ Distance Matrices with a Convolutional Neural Network for\n",
    "# Activity Recognition' paper\n",
    "def define_model_4():\n",
    "    model = Sequential()\n",
    "    ki = initializers.RandomNormal()\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(image_size,image_size,num_channels),\n",
    "                     kernel_initializer=ki))\n",
    "    model.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer=ki))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer=ki))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(optimizer=optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_middle(filenames, labels):\n",
    "    ta = zip(filenames, labels)\n",
    "    ta = np.array([(t, l) for t, l in ta], dtype=[('fname', 'S100'), ('label', 'int8')])\n",
    "    ta = ta[:][(ta['label'] == -1) | (ta['label'] == 1)]\n",
    "    ta['label'][ta['label'] == -1] = 0\n",
    "    return ta['fname'].astype(str, copy=False), ta['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall distribution:  {0: 129219, 1: 114114}\n",
      "Training distribution:  {0: 90425, 1: 79908}\n",
      "Testing distribution:  {0: 38794, 1: 34206}\n"
     ]
    }
   ],
   "source": [
    "fb = files_builder(DataKind('recplot'), subfolder=('sep_channels',))\n",
    "seed = 123\n",
    "fns = [fn[1] for fn in fb.file_names(include_path=True, subfolder=(), recursive=True)]\n",
    "filenames, labels = remove_middle(fns, fb.get_labels(fns))\n",
    "# labels = to_categorical(labels)\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "print('Overall distribution: ', dict(zip(unique, counts)))\n",
    "training_filenames, validation_filenames, training_labels, validation_labels = \\\n",
    "    train_test_split(filenames, labels, test_size=0.3, random_state=seed)\n",
    "assert len(training_filenames) == len(training_labels)\n",
    "assert len(validation_filenames) == len(validation_labels)\n",
    "unique, counts = np.unique(training_labels, return_counts=True)\n",
    "print('Training distribution: ', dict(zip(unique, counts)))\n",
    "unique, counts = np.unique(validation_labels, return_counts=True)\n",
    "print('Testing distribution: ', dict(zip(unique, counts)))\n",
    "training_labels = to_categorical(training_labels, dtype=training_labels.dtype)\n",
    "validation_labels = to_categorical(validation_labels, dtype=validation_labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n"
     ]
    }
   ],
   "source": [
    "training_batch_generator = batch_generator(training_filenames, training_labels, batch_size)\n",
    "validation_batch_generator = batch_generator(validation_filenames, validation_labels, batch_size)\n",
    "\n",
    "model = define_model_4()\n",
    "model.fit_generator(generator=training_batch_generator,\n",
    "                                      # steps_per_epoch=(len(training_filenames) // batch_size),\n",
    "                                      steps_per_epoch=len(training_batch_generator),\n",
    "                                      epochs=num_epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=validation_batch_generator,\n",
    "                                      # validation_steps=(len(validation_filenames) // batch_size),\n",
    "                                      validation_steps=len(validation_batch_generator),\n",
    "                                      use_multiprocessing=True,\n",
    "                                      workers=8,\n",
    "                                      max_queue_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total paramaters: 60,982,770.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.079"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_memory_usage(batch_size, model):\n",
    "    import numpy as np\n",
    "    from keras import backend as K\n",
    "\n",
    "    shapes_mem_count = 0\n",
    "    for l in model.layers:\n",
    "        single_layer_mem = 1\n",
    "        for s in l.output_shape:\n",
    "            if s is None:\n",
    "                continue\n",
    "            single_layer_mem *= s\n",
    "        shapes_mem_count += single_layer_mem\n",
    "\n",
    "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
    "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
    "\n",
    "    number_size = 4.0\n",
    "    if K.floatx() == 'float16':\n",
    "         number_size = 2.0\n",
    "    if K.floatx() == 'float64':\n",
    "         number_size = 8.0\n",
    "\n",
    "    print('Total paramaters: {:,}'.format(trainable_count + non_trainable_count))\n",
    "    total_memory = number_size*(batch_size*shapes_mem_count + trainable_count + non_trainable_count)\n",
    "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
    "    return gbytes\n",
    "get_model_memory_usage(128, define_model_4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_4/kernel:0' shape=(3, 3, 1, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_4/bias:0' shape=(32,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_5/kernel:0' shape=(3, 3, 32, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_5/bias:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_6/kernel:0' shape=(3, 3, 16, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2d_6/bias:0' shape=(8,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3/kernel:0' shape=(7200, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3/bias:0' shape=(8,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/kernel:0' shape=(8, 2) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4/bias:0' shape=(2,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model_1()\n",
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "K.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
